## 202 作业框架

## 作业零

从`index.html`看起里面的引入框架按编写框架引入

###`index.html`

```javascript
<!DOCTYPE html>
<html>

<head>
    <style>
        html,
        body {
            margin: 0;
            background-color: black;
            height: 100%;
            width: 100%;
            overflow: hidden;
        }

        #glcanvas {
            top: 0;
            width: 100%;
            height: 100%;
        }
    </style>
    <script src="lib/three.js" defer></script>
    <script src="lib/OrbitControls.js" defer></script>
    <script type="text/javascript" src="lib/MTLLoader.js" defer></script>
    <script type="text/javascript" src="lib/OBJLoader.js" defer></script>
    <script type="text/javascript" src="lib/dat.gui.js" defer></script>
    <script src="src/gl-matrix-min.js" defer> </script>

    <script src="src/shaders/InternalShader.js" defer></script>
    <script src="src/shaders/Shader.js" defer></script>
    <script src="src/materials/Material.js" defer></script>
    <script src="src/materials/PhongMaterial.js" defer></script>
    <script src="src/textures/Texture.js" defer></script>

    <script src="src/objects/Mesh.js" defer></script>
    <script src="src/loads/loadOBJ.js" defer></script>
    <script src="src/loads/loadShader.js" defer></script>

    <script src="src/lights/Light.js" defer></script>
    <script src="src/lights/PointLight.js" defer></script>

    <script src="src/renderers/WebGLRenderer.js" defer></script>
    <script src="src/renderers/MeshRender.js" defer></script>
    <script src="src/engine.js" defer></script>
</head>

<body>
    <canvas id="glcanvas"></canvas>

</body>

</html>
```

在提供的HTML片段中，通过`<script>`标签引入了六个JavaScript库或脚本文件，它们各自的作用如下：

1. **`three.js`**:
   - `three.js` 是一个轻量级的3D图形库，用于在网页上创建和显示3D图形内容。它提供了一套简洁的API，用于创建3D场景、相机、光源、物体、材质等，并且能够在WebGL渲染器上运行。

2. **`OrbitControls.js`**:
   - 这是`three.js`库的一个附加模块，提供了一种控制方式，使用户可以通过鼠标操作来旋转、缩放和平移三维场景中的相机。这对于交互式3D应用程序是非常有用的。

3. **`MTLLoader.js` 和 `OBJLoader.js`**:
   - 这两个脚本是`three.js`的扩展，用于加载特定格式的3D模型。
   - `MTLLoader.js` 用于加载`.mtl`文件，这种文件格式通常包含3D模型的材料信息，如纹理、颜色和反光度等。
   - `OBJLoader.js` 用于加载`.obj`文件，这是一种常见的3D模型格式，用于表示3D物体的几何形状。

4. **`dat.gui.js`**:
   - `dat.GUI` 是一个轻量级的图形用户界面库，用于在Web应用程序中添加交互式的控制面板。它常用于调整参数、配置设置和进行实时可视化操作，非常适合用于调试和演示3D图形应用程序。

5. **`gl-matrix-min.js`**:
   - `gl-matrix` 是一个高性能的矩阵和向量运算库，专门为WebGL和其他计算密集型JavaScript应用程序设计。它提供了各种操作2D和3D向量、矩阵等数学对象的功能，对于3D图形编程来说非常重要。

这些文件共同构成了一个基于Web的3D图形应用程序的基础架构，提供了从基本的3D渲染到复杂的用户交互功能所需的所有工具。通过结合这些库，可以构建功能丰富的3D体验和应用程序。

### 关于`three.js`

什么是Three.js？

Three.js是一款webGL框架，由于其易用性被广泛应用。Three.js在WebGL的api接口基础上，又进行的一层封装。hree.js以简单、直观的方式封装了3D图形编程中常用的对象。Three.js在开发中使用了很多图形引擎的高级技巧，极大地提高了性能。另外，由于内置了很多常用对象和极易上手的工具，Three.js的功能也非常强大。

Three.js作为WebGL框架中的佼佼者，由于它的易用性和扩展性，使得它能够满足大部分的开发需求，Three.js的具体功能如下：

- Three.js掩盖了3D渲染的细节：Three.js将WebGL原生API的细节抽象化，**将3D场景拆解为网格、材质和光源(即它内置了图形编程常用的一些对象种类)**。
- 面向对象：开发者可以使用上层的JavaScript对象，而不是仅仅调用JavaScript函数。
- 功能非常丰富：Three.js除了封装了WebGL原始API之外，Three.js还包含了许多实用的内置对象，可以方便地应用于游戏开发、动画制作、幻灯片制作、髙分辨率模型和一些特殊的视觉效果制作。
- 速度很快：Three.js采用了3D图形最佳实践来保证在不失可用性的前提下，保持极高的性能。
- 支持交互：WebGL本身并不提供拾取（picking)功能（即是否知道鼠标正处于某个物体上）。而Three.js则固化了拾取支持，这就使得你可以轻松为你的应用添加交互功能。
- 包含数学库：Three.js拥有一个强大易用的数学库，你可以在其中进行矩阵、投影和矢量运算。
- **内置文件格式支持：你可以使用流行的3D建模软件导出文本格式的文件，然后使用Three.js加载；也可以使用Three.js自己的JSON格式或二进制格式。**
- 扩展性很强：为Three.js添加新的特性或进行自定义优化是很容易的事情。如果你需要某个特殊的数据结构，那么只需要封装到Three.js即可。
- 支持HTML5 canvas：Three.js不但支持WebGL，而且还支持使用Canvas2D、Css3D和SVG进行渲染。在未兼容WebGL的环境中可以回退到其它的解决方案。

-----

在您提供的比喻中，将WebGL与Three.js的关系比作JavaScript与jQuery的关系，是为了说明一个底层技术与其上层库或框架之间的关系。

1. **WebGL与Three.js的关系**:
   - **WebGL** 是一个低层次的、基于OpenGL ES的Web图形API，它允许在Web浏览器中直接使用GPU进行3D渲染。WebGL提供了非常基础和强大的功能，但直接使用WebGL编写3D应用程序通常比较复杂和繁琐。
   - **Three.js** 是一个建立在WebGL之上的高层次3D图形库。它提供了更简单、更抽象的API来创建和管理3D场景。通过使用Three.js，开发者可以更容易地开发3D应用程序，而不必深入研究WebGL的复杂细节。

2. **JavaScript与jQuery的关系**:
   - **JavaScript** 是一种高级的编程语言，广泛用于网页开发。它是Web开发的基础，提供了创建动态网页内容的能力。
   - **jQuery** 是一个建立在JavaScript之上的库，旨在简化HTML文档的遍历、事件处理、动画和Ajax交互。jQuery为常见任务提供了更简单的语法，使得JavaScript编程更加方便和高效。

在这两种情况下，高层次的库（Three.js和jQuery）都提供了一种更简单、更易于使用的方式来利用底层技术（WebGL和JavaScript）。这些库的目的是抽象和简化底层技术的复杂性，使得开发者可以更快速、更有效地开发应用程序。

----

WebGL的完整工作流程

**1、准备数据阶段**
在这个阶段，我们需要提供顶点坐标、索引（三角形绘制顺序）、uv（决定贴图坐标）、法线（决定光照效果），以及各种矩阵（比如投影矩阵）。
其中顶点数据存储在缓存区（因为数量巨大），以修饰符attribute传递给顶点着色器；
矩阵则以修饰符uniform传递给顶点着色器。
**2、生成顶点着色器**
根据我们需要，由Javascript定义一段顶点着色器（opengl es）程序的字符串，生成并且编译成一段着色器程序传递给GPU。
**3、图元装配**
GPU根据顶点数量，挨个执行顶点着色器程序，生成顶点最终的坐标，完成坐标转换。
**4、生成片元着色器**
模型是什么颜色，看起来是什么质地，光照效果，阴影（流程较复杂，需要先渲染到纹理，可以先不关注），都在这个阶段处理。
**5、光栅化**
能过片元着色器，我们确定好了每个片元的颜色，以及根据深度缓存区判断哪些片元被挡住了，不需要渲染，最终将片元信息存储到颜色缓存区，最终完成整个渲染。

![img](https://images2015.cnblogs.com/blog/111077/201704/111077-20170424111332459-1733042331.png)

**Three.js究竟做了什么？**

我们知道，three.js帮我们完成了很多事情，但是它具体做了什么呢，他在整个流程中，扮演了什么角色呢？
我们先简单看一下，three.js参与的流程：

![img](https://images2015.cnblogs.com/blog/111077/201704/111077-20170424111758709-1896451652.png)

黄色和绿色部分，都是three.js参与的部分，其中黄色是javascript部分，绿色是opengl es部分。
我们发现，能做的，three.js基本上都帮我们做了。

- 辅助我们导出了模型数据；
- 自动生成了各种矩阵；
- 生成了顶点着色器；
- 辅助我们生成材质，配置灯光；
- 根据我们设置的材质生成了片元着色器。

而且将webGL基于光栅化的2D API，封装成了我们人类能看懂的 3D API。

----

OrbitControls.js

three.js 的相机控件，可以实现场景的缩放、旋转等操作，之所以不在 three.js 中，而是单独存在于一个 js 文件中，是因为这个存在于 three.js 的 example 目录下的实例代码，并不属于 three.js 的发布库。

### `InternalShader.js`

包含了所用到的所有Shader源码

### `Shader.js`

```javascript
class Shader {

    constructor(gl, vsSrc, fsSrc, shaderLocations) {
        this.gl = gl;
        const vs = this.compileShader(vsSrc, gl.VERTEX_SHADER);
        const fs = this.compileShader(fsSrc, gl.FRAGMENT_SHADER);

        this.program = this.addShaderLocations({
            glShaderProgram: this.linkShader(vs, fs),
        }, shaderLocations);
    }

    compileShader(shaderSource, shaderType) {
        const gl = this.gl;
        var shader = gl.createShader(shaderType);
        gl.shaderSource(shader, shaderSource);
        gl.compileShader(shader);

        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
            console.error(shaderSource);
            console.error(('shader compiler error:\n' + gl.getShaderInfoLog(shader)));
        }

        return shader;
    };

    linkShader(vs, fs) {
        const gl = this.gl;
        var prog = gl.createProgram();
        gl.attachShader(prog, vs);
        gl.attachShader(prog, fs);
        gl.linkProgram(prog);

        if (!gl.getProgramParameter(prog, gl.LINK_STATUS)) {
            abort('shader linker error:\n' + gl.getProgramInfoLog(prog));
        }
        return prog;
    };

    addShaderLocations(result, shaderLocations) {
        const gl = this.gl;
        result.uniforms = {};
        result.attribs = {};

        if (shaderLocations && shaderLocations.uniforms && shaderLocations.uniforms.length) {
            for (let i = 0; i < shaderLocations.uniforms.length; ++i) {
                result.uniforms = Object.assign(result.uniforms, {
                    [shaderLocations.uniforms[i]]: gl.getUniformLocation(result.glShaderProgram, shaderLocations.uniforms[i]),
                });
                //console.log(gl.getUniformLocation(result.glShaderProgram, 'uKd'));
            }
        }
        if (shaderLocations && shaderLocations.attribs && shaderLocations.attribs.length) {
            for (let i = 0; i < shaderLocations.attribs.length; ++i) {
                result.attribs = Object.assign(result.attribs, {
                    [shaderLocations.attribs[i]]: gl.getAttribLocation(result.glShaderProgram, shaderLocations.attribs[i]),
                });
            }
        }
        
        return result;
    }
}

```

其中包含的OpenGL语法有

这段代码使用了WebGL API，这是一个基于OpenGL ES（一个用于嵌入式系统的OpenGL子集）的Web标准。WebGL提供了一种在网页浏览器中使用硬件加速的3D图形的方法。下面是这个`Shader`类中使用的一些主要WebGL API，以及它们的作用和返回值：

1. **`gl.createShader(shaderType)`**:
   - 功能：创建一个新的着色器对象。
   - 参数：`shaderType` 指定着色器类型（`gl.VERTEX_SHADER` 或 `gl.FRAGMENT_SHADER`）。
   - 返回值：一个新的着色器对象。

2. **`gl.shaderSource(shader, shaderSource)`**:
   - 功能：设置着色器的源代码。
   - 参数：`shader` 是着色器对象，`shaderSource` 是包含着色器代码的字符串。
   - 返回值：无。

3. **`gl.compileShader(shader)`**:
   - 功能：编译着色器。
   - 参数：`shader` 是要编译的着色器对象。
   - 返回值：无。

4. **`gl.getShaderParameter(shader, gl.COMPILE_STATUS)`**:
   - 功能：获取着色器的编译状态。
   - 参数：`shader` 是着色器对象，`gl.COMPILE_STATUS` 是要查询的参数。
   - 返回值：一个布尔值，表示是否成功编译。

5. **`gl.createProgram()`**:
   - 功能：创建一个新的着色器程序对象。
   - 返回值：一个新的程序对象。

6. **`gl.attachShader(program, shader)`**:
   - 功能：将着色器附加到程序上。
   - 参数：`program` 是程序对象，`shader` 是着色器对象。
   - 返回值：无。

7. **`gl.linkProgram(program)`**:
   - 功能：链接程序中的着色器。
   - 参数：`program` 是要链接的程序对象。
   - 返回值：无。

8. **`gl.getProgramParameter(program, gl.LINK_STATUS)`**:
   - 功能：获取程序的链接状态。
   - 参数：`program` 是程序对象，`gl.LINK_STATUS` 是要查询的参数。
   - 返回值：一个布尔值，表示是否成功链接。

9. **`gl.getUniformLocation(program, name)`** 和 **`gl.getAttribLocation(program, name)`**:
   - 功能：获取uniform变量或attribute变量在着色器程序中的位置。
   - 参数：`program` 是程序对象，`name` 是变量的名称。
   - 返回值：对于 `getUniformLocation`，返回uniform位置的引用；对于 `getAttribLocation`，返回attribute位置的整数索引。

这些API在WebGL编程中是非常基础且常用的，用于创建、编译、链接着色器，并将它们组合成着色器程序，用于WebGL渲染过程。这些函数的返回值通常用于之后的WebGL渲染调用，以便对顶点数据、纹理等进行处理。

---

对于一个Javascript语法Obeject.assign使用：

当然，让我们更详细地解析这行使用 `Object.assign` 的代码：

```javascript
result.uniforms = Object.assign(result.uniforms, {
    [shaderLocations.uniforms[i]]: gl.getUniformLocation(result.glShaderProgram, shaderLocations.uniforms[i]),
});
```

这行代码的作用是向 `result.uniforms` 对象中添加新的属性。下面是具体的解释：

1. **`Object.assign` 函数**:
   - `Object.assign` 是JavaScript的一个内置函数，用于将所有可枚举属性从一个或多个源对象复制到目标对象，并返回目标对象。在这个例子中，它被用来更新 `result.uniforms` 对象。

2. **目标对象 (`result.uniforms`)**:
   - `result.uniforms` 是目标对象，即该行代码执行后将被更新的对象。

3. **源对象**:
   - `{ [shaderLocations.uniforms[i]]: gl.getUniformLocation(result.glShaderProgram, shaderLocations.uniforms[i]) }` 是源对象，其中包含了一个属性。
   - `[shaderLocations.uniforms[i]]` **使用了ES6的计算属性名语法**，**这意味着属性名是动态计算的**。在这个例子中，属性名是 `shaderLocations.uniforms` 数组中当前遍历到的元素（一个uniform变量的名称）。
   - `gl.getUniformLocation(result.glShaderProgram, shaderLocations.uniforms[i])` 是调用WebGL API获取的着色器程序中对应uniform变量的位置。

4. **操作结果**:
   - 这行代码的执行结果是 `result.uniforms` 对象被更新。对于数组 `shaderLocations.uniforms` 中的每个元素（uniform变量名），该代码都会向 `result.uniforms` 对象添加一个新属性，属性名是uniform变量名，属性值是该uniform变量在着色器程序中的位置。

简而言之，这行代码在循环中，对于着色器程序中的每个uniform变量，都在 `result.uniforms` 对象中动态添加一个新属性，其中属性名是uniform变量的名字，属性值是该变量在着色器程序中的位置。这使得在后续的渲染过程中可以轻松地通过这些名称访问和设置uniform变量的值。

----

在AddLocation中不用返回以及链接好的OpenGL程序的原因：

着色器程序（`program`）本身，在 `linkShader` 方法中已经被创建和链接。一旦链接成功，着色器程序就已经在WebGL上下文（`gl`）中完成了设置，并且可以被激活和使用。在 `addShaderLocations` 方法中不需要再返回它，因为着色器程序已经在之前的步骤中被创建并准备好了。

### `Material.js`

```javascript
class Material {
    #flatten_uniforms;
    #flatten_attribs;
    #vsSrc;
    #fsSrc;
    // Uniforms is a map, attribs is a Array
    constructor(uniforms, attribs, vsSrc, fsSrc) {
        this.uniforms = uniforms;
        this.attribs = attribs;
        this.#vsSrc = vsSrc;
        this.#fsSrc = fsSrc;
        
        this.#flatten_uniforms = ['uModelViewMatrix', 'uProjectionMatrix', 'uCameraPos', 'uLightPos'];
        for (let k in uniforms) {
            this.#flatten_uniforms.push(k);
        }
        this.#flatten_attribs = attribs;
    }

    setMeshAttribs(extraAttribs) {
        for (let i = 0; i < extraAttribs.length; i++) {
            this.#flatten_attribs.push(extraAttribs[i]);
        }
    }

    compile(gl) {
        return new Shader(gl, this.#vsSrc, this.#fsSrc,
            {
                uniforms: this.#flatten_uniforms,
                attribs: this.#flatten_attribs
            });
    }
}
```

这里包含的Javascript语法：

1. \#号申明的是私有成员
2. flatten_uniforms将uniform矩阵展平
3. flatten_attribus将所有的attribute展平
4. 为什么要展平：在compile中创建一个新的Shader对象，需要将展平的数据以ShaderLocation传入

### `PhongMaterial.js`

```javascript
class PhongMaterial extends Material {
    
    /**
    * Creates an instance of PhongMaterial.
    * @param {vec3f} color The material color
    * @param {Texture} colorMap The texture object of the material
    * @param {vec3f} specular The material specular coefficient
    * @param {float} intensity The light intensity
    * @memberof PhongMaterial
    */
    constructor(color , colorMap , specular , intensity) 
    {

    let textureSample = 0;

    if (colorMap != null) {
    textureSample = 1;
    super(
        {
            'uTextureSample': { type: '1i', value: textureSample },
            'uSampler': { type: 'texture', value: colorMap },
            'uKd': { type: '3fv', value: color },
            'uKs': { type: '3fv', value: specular },
            'uLightIntensity': { type: '1f', value: intensity }
        }, [], PhongVertexShader , PhongFragmentShader
        );
     } 
     else {
     //console.log(color);
     super({
        'uTextureSample': { type: '1i', value: textureSample },
        'uKd': { type: '3fv', value: color },
        'uKs': { type: '3fv', value: specular },
        'uLightIntensity': { type: '1f', value: intensity }
     }, [], PhongVertexShader , PhongFragmentShader);
         }
    
     }
            }  
```

1. 其中使用父类构造函数，传入参数对应的是
   - uniforms为花括号定义的对象
   - []传入的attribute为空
   - 传入在InernalShader中定义的Phong顶点源码和Phong片段着色器源码
   - utextureSample表示有无纹理

### `Texture.js`

```javascript
class Texture {
    constructor(gl, img) {
        this.texture = gl.createTexture();
        gl.bindTexture(gl.TEXTURE_2D, this.texture);

        // Because images have to be download over the internet
        // they might take a moment until they are ready.
        // Until then put a single pixel in the texture so we can
        // use it immediately. When the image has finished downloading
        // we'll update the texture with the contents of the image.
        const level = 0;
        const internalFormat = gl.RGBA;
        const width = 1;
        const height = 1;
        const border = 0;
        const srcFormat = gl.RGBA;
        const srcType = gl.UNSIGNED_BYTE;
        const pixel = new Uint8Array([0, 0, 255, 255]); // opaque blue
        gl.texImage2D(gl.TEXTURE_2D, level, internalFormat,
            width, height, border, srcFormat, srcType,
            pixel);

        gl.bindTexture(gl.TEXTURE_2D, this.texture);
        gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);
        gl.texImage2D(gl.TEXTURE_2D, level, internalFormat,
            srcFormat, srcType, img);

        // WebGL1 has different requirements for power of 2 images
        // vs non power of 2 images so check if the image is a
        // power of 2 in both dimensions.
        if (isPowerOf2(img.width) && isPowerOf2(img.height)) {
            // Yes, it's a power of 2. Generate mips.
            gl.generateMipmap(gl.TEXTURE_2D);
        } else {
            // No, it's not a power of 2. Turn of mips and set
            // wrapping to clamp to edge
            //gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.REPEATE);
            //gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.REPEATE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        }
        gl.bindTexture(gl.TEXTURE_2D, null);
    }
}

function isPowerOf2(value) {
    return (value & (value - 1)) == 0;
}

```

这个 `Texture` 类在WebGL上下文中用于创建和管理纹理。它的主要功能是将图像数据加载到WebGL纹理中，以便在3D渲染中使用。以下是对这个类和其中方法的详细解释：

1. **构造函数 (`constructor`)**:
   - 接收 `gl`（WebGL上下文）和 `img`（图像数据）作为参数。
   - 创建一个新的WebGL纹理对象并将其绑定到 `gl.TEXTURE_2D` 目标上。
   - 初始时，将一个1x1大小的蓝色像素作为纹理的临时内容，这样做的目的是在图像数据还未完全加载时提供一个占位符。
   - 当图像数据加载完成后，使用该图像更新纹理。
   - 根据图像的尺寸是否是2的幂次方，决定是否生成MIP映射（Mipmap）或设置纹理参数（如包裹方式和过滤器）。

2. **纹理设置**:
   - 使用 `gl.texImage2D` 将图像数据上传到纹理。
   - 如果图像的尺寸是2的幂次方（`isPowerOf2` 函数检测），则生成MIP映射，这有助于提高纹理在不同尺度下的渲染效果。
   - 如果图像不是2的幂次方，设置纹理的包裹方式（`gl.TEXTURE_WRAP_S` 和 `gl.TEXTURE_WRAP_T`）和过滤器（`gl.TEXTURE_MIN_FILTER` 和 `gl.TEXTURE_MAG_FILTER`），以适应非2的幂次方的纹理。

3. **`isPowerOf2` 函数**:
   - 检测一个数值是否是2的幂次方，这对于确定纹理处理策略（如是否生成MIP映射）很重要。

这个类在3D图形应用中非常重要，因为纹理是增加物体表面细节和视觉效果的关键元素。通过将图像数据正确地加载到WebGL纹理中，`Texture` 类使得3D物体可以被更加真实和详细地渲染。

在提供的 `Texture` 类中，使用了多个WebGL API，每个API都有特定的输入（参数）和输出（返回值或效果）。以下是对这些主要函数的输入和输出的概述：

1. **`gl.createTexture()`**:
   - 输入: 无。
   - 输出: 返回一个新的WebGL纹理对象。

2. **`gl.bindTexture(target, texture)`**:
   - 输入: 
     - `target`：指定绑定的目标（在这种情况下为 `gl.TEXTURE_2D`）。
     - `texture`：要绑定的纹理对象。
   - 输出: 将指定的纹理对象绑定到指定的目标。

3. **`gl.texImage2D(target, level, internalformat, width, height, border, format, type, pixels)`**:
   - 输入: 
     - `target`：指定纹理目标（`gl.TEXTURE_2D`）。
     - `level`：指定详细级别（通常为0，代表基本图像级别）。
     - `internalformat`、`format`：纹理数据的格式（如 `gl.RGBA`）。
     - `width`、`height`：纹理图像的尺寸。
     - `border`：纹理边框的大小（通常为0）。
     - `type`：指定像素数据的数据类型（如 `gl.UNSIGNED_BYTE`）。
     - `pixels`：包含纹理图像的像素数据。
   - 输出: 在WebGL上下文中指定的纹理目标上创建或更新纹理图像。

4. **`gl.pixelStorei(pname, param)`**:
   - 输入: 
     - `pname`：指定要设置的参数（如 `gl.UNPACK_FLIP_Y_WEBGL`）。
     - `param`：参数的值（如 `true` 或 `false`）。
   - 输出: 设置纹理像素存储模式。

5. **`gl.generateMipmap(target)`**:
   - 输入: 
     - `target`：指定生成MIP映射的纹理目标（`gl.TEXTURE_2D`）。
   - 输出: 为指定的纹理目标生成MIP映射。

6. **`gl.texParameteri(target, pname, param)`**:
   - 输入: 
     - `target`：指定要设置的纹理目标（如 `gl.TEXTURE_2D`）。
     - `pname`：指定要设置的参数名称（如 `gl.TEXTURE_WRAP_S`）。
     - `param`：指定参数的值（如 `gl.CLAMP_TO_EDGE` 或 `gl.LINEAR`）。
   - 输出: 设置纹理的参数。

7. **`isPowerOf2(value)`**:
   - 输入: 
     - `value`：要检查的数值。
   - 输出: 如果 `value` 是2的幂次方，则返回 `true`；否则返回 `false`。

这些函数共同用于在WebGL中管理纹理的创建、配置和更新。通过正确使用这些API，可以确保纹理能够被有效地应用于3D渲染中的物体表面。

----

这里包含的OpenGL的接口文档：

一个可行的生成纹理的过程

```c++
unsigned int texture;
glGenTextures(1, &texture);
glBindTexture(GL_TEXTURE_2D, texture);
// 为当前绑定的纹理对象设置环绕、过滤方式
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);   
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
// 加载并生成纹理
int width, height, nrChannels;
unsigned char *data = stbi_load("container.jpg", &width, &height, &nrChannels, 0);
if (data)
{
    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);
    glGenerateMipmap(GL_TEXTURE_2D);
}
else
{
    std::cout << "Failed to load texture" << std::endl;
}
stbi_image_free(data);
```

①`glTexParameteri`：设置纹理对象的参数

- 第一个参数指定了纹理目标；我们使用的是2D纹理，因此纹理目标是GL_TEXTURE_2D

- 第二个参数需要我们指定设置的选项与应用的纹理轴。我们打算配置的是`WRAP`选项，并且指定`S`和`T`轴。（如果是使用3D纹理那么还有一个`r`）它们和`x`、`y`、`z`是等价的）

- 最后一个参数需要我们传递一个环绕方式(Wrapping)，在这个例子中OpenGL会给当前激活的纹理设定纹理环绕方式为GL_MIRRORED_REPEAT

  - | 环绕方式           | 描述                                                         |
    | :----------------- | :----------------------------------------------------------- |
    | GL_REPEAT          | 对纹理的默认行为。重复纹理图像。                             |
    | GL_MIRRORED_REPEAT | 和GL_REPEAT一样，但每次重复图片是镜像放置的。                |
    | GL_CLAMP_TO_EDGE   | 纹理坐标会被约束在0到1之间，超出的部分会重复纹理坐标的边缘，产生一种边缘被拉伸的效果。 |
    | GL_CLAMP_TO_BORDER | 超出的坐标为用户指定的边缘颜色。还需要手动设置一个Border的颜色 |

  - ![img](https://learnopengl-cn.github.io/img/01/06/texture_wrapping.png)

- 当进行放大(Magnify)和缩小(Minify)操作的时候可以设置纹理过滤的选项，比如你可以在纹理被缩小的时候使用邻近过滤，被放大时使用线性过滤。我们需要使用glTexParameter*函数为放大和缩小指定过滤方式。这段代码看起来会和纹理环绕方式的设置很相似：

  ```c++
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
  ```
  
  ![img](https://learnopengl-cn.github.io/img/01/06/texture_filtering.png)
  

②生成纹理

和之前生成的OpenGL对象一样，纹理也是使用ID引用的。让我们来创建一个：

`unsigned int texture; `

`glGenTextures(1, &texture);`

glGenTextures函数首先需要输入生成纹理的数量，然后把它们储存在第二个参数的`unsigned int`数组中（我们的例子中只是单独的一个`unsigned int`），就像其他对象一样，我们需要绑定它，让之后任何的纹理指令都可以配置当前绑定的纹理：

```c++
glBindTexture(GL_TEXTURE_2D, texture);
```

现在纹理已经绑定了，我们可以使用前面载入的图片数据生成一个纹理了。纹理可以通过glTexImage2D来生成：

```c++
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);
glGenerateMipmap(GL_TEXTURE_2D);
```

函数很长，参数也不少，所以我们一个一个地讲解：

- 第一个参数指定了纹理目标(Target)。设置为GL_TEXTURE_2D意味着会生成与当前绑定的纹理对象在同一个目标上的纹理（任何绑定到GL_TEXTURE_1D和GL_TEXTURE_3D的纹理不会受到影响）。

- 第二个参数为纹理指定多级渐远纹理的级别(也就是Mipmap的插值方式），如果你希望单独手动设置每个多级渐远纹理的级别的话。这里我们填0，也就是基本级别。

  | 过滤方式                      | 描述                                                         |
  | :---------------------------- | :----------------------------------------------------------- |
  | GL_NEAREST_MIPMAP_NEAREST（0) | 使用最邻近的多级渐远纹理来匹配像素大小，并使用邻近插值进行纹理采样 |
  | GL_LINEAR_MIPMAP_NEAREST(1)   | 使用最邻近的多级渐远纹理级别，并使用线性插值进行采样         |
  | GL_NEAREST_MIPMAP_LINEAR(2)   | 在两个最匹配像素大小的多级渐远纹理之间进行线性插值，使用邻近插值进行采样 |
  | GL_LINEAR_MIPMAP_LINEAR(3)    | 在两个邻近的多级渐远纹理之间使用线性插值，并使用线性插值进行采样 |

- 第三个参数告诉OpenGL我们希望把纹理储存为何种格式。我们的图像只有`RGB`值，因此我们也把纹理储存为`RGB`值。
- 第四个和第五个参数设置最终的纹理的宽度和高度。我们之前加载图像的时候储存了它们，所以我们使用对应的变量。
- 下个参数应该总是被设为`0`（历史遗留的问题）。
- 第七第八个参数定义了源图的格式和数据类型。我们使用RGB值加载这个图像，并把它们储存为`char`(byte)数组，我们将会传入对应值。
- 最后一个参数是真正的图像数据。

**当调用glTexImage2D时，当前绑定的纹理对象就会被附加上纹理图像**。然而，目前只有基本级别(Base-level)的纹理图像被加载了，如果要使用多级渐远纹理，我们必须手动设置所有不同的图像（不断递增第二个参数）。或者，直接在生成纹理之后调用glGenerateMipmap。这会为当前绑定的纹理自动生成所有需要的多级渐远纹理。

---

具体接口介绍参考

[纹理 - LearnOpenGL CN (learnopengl-cn.github.io)](https://learnopengl-cn.github.io/01 Getting started/06 Textures/#_6)

### `Mesh.js`

`Mesh` 类在3D图形编程中用于表示一个网格对象。网格是由顶点、边和面组成的3D对象的基本数据结构，它们通常用于定义3D模型的形状。在这个 `Mesh` 类中，它主要负责存储和管理与一个3D网格相关的数据，如顶点坐标、法线、纹理坐标和索引。下面是对这个类的功能的具体解释：

1. **构造函数 (`constructor`)**:
   - 接收顶点属性、法线属性、纹理坐标属性和索引作为参数。
   - 根据这些参数，构造函数初始化网格对象的各种属性，包括顶点数组 (`vertices`)、法线数组 (`normals`)、纹理坐标数组 (`texcoords`) 和索引数组 (`indices`)。
   - 还记录了哪些数据是可用的（例如，如果提供了顶点数据，则 `hasVertices` 为 `true`）。

2. **静态方法 `cube`**:
   - `cube` 是一个静态方法，用于创建一个立方体网格。
   - 它定义了立方体的顶点位置和索引（用于定义立方体的每个面）。
   - 这个方法返回一个新的 `Mesh` 实例，该实例代表一个立方体。

`Mesh` 类的主要作用是封装和管理构成3D物体的数据。在WebGL或其他3D图形库中，这些数据通常需要被上传到GPU，并用于在渲染过程中绘制物体。例如，顶点数据定义了物体的形状，法线数据可以用于光照计算，纹理坐标定义了如何将纹理映射到物体表面。

这样的类设计使得处理3D图形时可以更加组织化和模块化，使得开发者能够更容易地创建和操作复杂的3D场景。

----

`cube` 方法中的数字定义了一个立方体的顶点坐标。在3D图形中，一个立方体由6个面组成，每个面由4个顶点定义。因此，一个立方体总共需要8个独立的顶点，但由于在3D图形中通常使用三角形来表示面，所以每个面被分为两个三角形，每个三角形由3个顶点定义。在这个方法中，为了方便使用三角形，每个面的顶点被重复定义了一次。

以下是数字的具体含义：

- 立方体的每组3个数字代表空间中一个点的 `x`、`y` 和 `z` 坐标。
- 这些坐标按照立方体的各个面（前、后、上、下、右、左）进行了组织。
- 每个面由4个顶点定义，但由于使用了两个三角形来表示每个面，所以每个面的顶点在数组中出现了两次。

例如：

```javascript
// Front face
-1.0, -1.0, 1.0, // 第一个顶点的x, y, z坐标
1.0, -1.0, 1.0,  // 第二个顶点的x, y, z坐标
1.0, 1.0, 1.0,   // 第三个顶点的x, y, z坐标
-1.0, 1.0, 1.0,  // 第四个顶点的x, y, z坐标
```

这四个点定义了立方体的前面。由于这些点定义的是单位立方体（每条边长为2，中心在原点），所以坐标值为 `-1.0` 或 `1.0`。

`indices` 数组定义了用于绘制立方体的三角形的顶点索引。每组6个数字代表两个三角形，用于绘制立方体的一个面。

例如：

```javascript
0, 1, 2, 0, 2, 3, // front
```

这表示前面的两个三角形由顶点 0, 1, 2 和顶点 0, 2, 3 组成。

通过这种方式，`cube` 方法构建了一个完整的立方体网格，可以在WebGL等3D图形库中进行渲染。

```javascript
class Mesh {
	constructor(verticesAttrib, normalsAttrib, texcoordsAttrib, indices) {
		this.indices = indices;
		this.count = indices.length;
		this.hasVertices = false;
		this.hasNormals = false;
		this.hasTexcoords = false;
		let extraAttribs = [];

		if (verticesAttrib != null) {
			this.hasVertices = true;
			this.vertices = verticesAttrib.array;
			this.verticesName = verticesAttrib.name;
		}
		if (normalsAttrib != null) {
			this.hasNormals = true;
			this.normals = normalsAttrib.array;
			this.normalsName = normalsAttrib.name;
		}
		if (texcoordsAttrib != null) {
			this.hasTexcoords = true;
			this.texcoords = texcoordsAttrib.array;
			this.texcoordsName = texcoordsAttrib.name;
		}
	}

	static cube() {
		const positions = [
			// Front face
			-1.0, -1.0, 1.0, //顶点0
			1.0, -1.0, 1.0,  //顶点1
			1.0, 1.0, 1.0,	//顶点2
			-1.0, 1.0, 1.0,

			// Back face
			-1.0, -1.0, -1.0,
			-1.0, 1.0, -1.0,
			1.0, 1.0, -1.0,
			1.0, -1.0, -1.0,

			// Top face
			-1.0, 1.0, -1.0,
			-1.0, 1.0, 1.0,
			1.0, 1.0, 1.0,
			1.0, 1.0, -1.0,

			// Bottom face
			-1.0, -1.0, -1.0,
			1.0, -1.0, -1.0,
			1.0, -1.0, 1.0,
			-1.0, -1.0, 1.0,

			// Right face
			1.0, -1.0, -1.0,
			1.0, 1.0, -1.0,
			1.0, 1.0, 1.0,
			1.0, -1.0, 1.0,

			// Left face
			-1.0, -1.0, -1.0,
			-1.0, -1.0, 1.0,
			-1.0, 1.0, 1.0,
			-1.0, 1.0, -1.0,
		];
		const indices = [
			0, 1, 2, 0, 2, 3,    // front // 是顶点的索引，代表顶点0，1，2，0，2，3即两个三角形构成了一个面
			4, 5, 6, 4, 6, 7,    // back
			8, 9, 10, 8, 10, 11,   // top
			12, 13, 14, 12, 14, 15,   // bottom
			16, 17, 18, 16, 18, 19,   // right
			20, 21, 22, 20, 22, 23,   // left
		];
		return new Mesh({ name: 'aVertexPosition', array: new Float32Array(positions) }, null, null, indices);
	}
}
```



### `loadOBJ.js`

  这个函数充分利用了THREE.js提供的加载器和工具来加载和处理OBJ格式的3D模型及其相关材质，这在3D图形应用中是一个常见的操作。通过这种方式，可以将复杂的3D模型导入到WebGL场景中，并进行渲染和视觉展示。

----

看不懂加载原理（跳过）

### `loadShader.js`

### `light.js`

```javascript
class EmissiveMaterial extends Material {

    constructor(lightIntensity, lightColor) {    
        super({
            'uLigIntensity': { type: '1f', value: lightIntensity },
            'uLightColor': { type: '3fv', value: lightColor }
        }, [], LightCubeVertexShader, LightCubeFragmentShader);
        
        this.intensity = lightIntensity;
        this.color = lightColor;
    }
}
```

继承自一个材质，命名为会发光的材质。定义了一个光强和光的颜色属性

### `ponitlight.js`

```javascript

class PointLight {
    /**
     * Creates an instance of PointLight.
     * @param {float} lightIntensity  The intensity of the PointLight.
     * @param {vec3f} lightColor The color of the PointLight.
     * @memberof PointLight
     */
    constructor(lightIntensity, lightColor) {
        this.mesh = Mesh.cube();
        this.mat = new EmissiveMaterial(lightIntensity, lightColor);
    }
}
```

创建了一个立方体cude代表点光源本身

在将mat属性绑定到一个light对象上

### `MeshRender.js`

```javascript

class MeshRender {

	#vertexBuffer;
	#normalBuffer;
	#texcoordBuffer;
	#indicesBuffer;
	
	constructor(gl, mesh, material) {
		this.gl = gl;
		this.mesh = mesh;
		this.material = material;

		this.#vertexBuffer = gl.createBuffer();
		this.#normalBuffer = gl.createBuffer();
		this.#texcoordBuffer = gl.createBuffer();
		this.#indicesBuffer = gl.createBuffer();

		let extraAttribs = []
		if (mesh.hasVertices) {
			extraAttribs.push(mesh.verticesName);
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#vertexBuffer);
			gl.bufferData(gl.ARRAY_BUFFER, mesh.vertices, gl.STATIC_DRAW);
			gl.bindBuffer(gl.ARRAY_BUFFER, null);
		}

		if (mesh.hasNormals) {
			extraAttribs.push(mesh.normalsName);
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#normalBuffer);
			gl.bufferData(gl.ARRAY_BUFFER, mesh.normals, gl.STATIC_DRAW);
			gl.bindBuffer(gl.ARRAY_BUFFER, null);
		}

		if (mesh.hasTexcoords) {
			extraAttribs.push(mesh.texcoordsName);
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#texcoordBuffer);
			gl.bufferData(gl.ARRAY_BUFFER, mesh.texcoords, gl.STATIC_DRAW);
			gl.bindBuffer(gl.ARRAY_BUFFER, null);
		}

		gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, this.#indicesBuffer);
		gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(mesh.indices), gl.STATIC_DRAW);
		gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null);

		this.material.setMeshAttribs(extraAttribs);
		this.shader = this.material.compile(gl);
	}

	draw(camera, transform) {
		const gl = this.gl;

		let modelViewMatrix = mat4.create();
		let projectionMatrix = mat4.create();

		camera.updateMatrixWorld();
		mat4.invert(modelViewMatrix, camera.matrixWorld.elements);
		mat4.translate(modelViewMatrix, modelViewMatrix, transform.translate);
		mat4.scale(modelViewMatrix, modelViewMatrix, transform.scale);
		mat4.copy(projectionMatrix, camera.projectionMatrix.elements);

		if (this.mesh.hasVertices) {
			const numComponents = 3;
			const type = gl.FLOAT;
			const normalize = false;
			const stride = 0;
			const offset = 0;
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#vertexBuffer);
			gl.vertexAttribPointer(
				this.shader.program.attribs[this.mesh.verticesName],
				numComponents,
				type,
				normalize,
				stride,
				offset);
			gl.enableVertexAttribArray(
				this.shader.program.attribs[this.mesh.verticesName]);
		}

		if (this.mesh.hasNormals) {
			const numComponents = 3;
			const type = gl.FLOAT;
			const normalize = false;
			const stride = 0;
			const offset = 0;
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#normalBuffer);
			gl.vertexAttribPointer(
				this.shader.program.attribs[this.mesh.normalsName],
				numComponents,
				type,
				normalize,
				stride,
				offset);
			gl.enableVertexAttribArray(
				this.shader.program.attribs[this.mesh.normalsName]);
		}

		if (this.mesh.hasTexcoords) {
			const numComponents = 2;
			const type = gl.FLOAT;
			const normalize = false;
			const stride = 0;
			const offset = 0;
			gl.bindBuffer(gl.ARRAY_BUFFER, this.#texcoordBuffer);
			gl.vertexAttribPointer(
				this.shader.program.attribs[this.mesh.texcoordsName],
				numComponents,
				type,
				normalize,
				stride,
				offset);
			gl.enableVertexAttribArray(
				this.shader.program.attribs[this.mesh.texcoordsName]);
		}

		gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, this.#indicesBuffer);

		gl.useProgram(this.shader.program.glShaderProgram);

		gl.uniformMatrix4fv(
			this.shader.program.uniforms.uProjectionMatrix,
			false,
			projectionMatrix);
		gl.uniformMatrix4fv(
			this.shader.program.uniforms.uModelViewMatrix,
			false,
			modelViewMatrix);

		// Specific the camera uniforms
		gl.uniform3fv(
			this.shader.program.uniforms.uCameraPos,
			[camera.position.x, camera.position.y, camera.position.z]);

		for (let k in this.material.uniforms) {
			if (this.material.uniforms[k].type == 'matrix4fv') {
				gl.uniformMatrix4fv(
					this.shader.program.uniforms[k],
					false,
					this.material.uniforms[k].value);
			} else if (this.material.uniforms[k].type == '3fv') {
				gl.uniform3fv(
					this.shader.program.uniforms[k],
					this.material.uniforms[k].value);
			} else if (this.material.uniforms[k].type == '1f') {
				gl.uniform1f(
					this.shader.program.uniforms[k],
					this.material.uniforms[k].value);
			} else if (this.material.uniforms[k].type == '1i') {
				gl.uniform1i(
					this.shader.program.uniforms[k],
					this.material.uniforms[k].value);
			} else if (this.material.uniforms[k].type == 'texture') {
				gl.activeTexture(gl.TEXTURE0);
				gl.bindTexture(gl.TEXTURE_2D, this.material.uniforms[k].value.texture);
				gl.uniform1i(this.shader.program.uniforms[k], 0);
			}
		}

		{
			const vertexCount = this.mesh.count;
			const type = gl.UNSIGNED_SHORT;
			const offset = 0;
			gl.drawElements(gl.TRIANGLES, vertexCount, type, offset);
		}
	}
}
```

包含的OpenGL语法包括了

在提供的 `MeshRender` 类代码中，使用了多个WebGL API调用，这些API是基于OpenGL ES的Web标准。以下是代码中用到的关键WebGL（类似于OpenGL）的语法和函数：

1. **创建缓冲区**:
   - `gl.createBuffer()`：创建一个新的WebGL缓冲区对象。

2. **绑定缓冲区**:
   - `gl.bindBuffer(bufferType, buffer)`：将创建的缓冲区对象绑定到指定的缓冲类型（如 `gl.ARRAY_BUFFER` 或 `gl.ELEMENT_ARRAY_BUFFER`）。

3. **缓冲数据**:
   - `gl.bufferData(bufferType, data, usage)`：将顶点数据（如位置、法线、纹理坐标）或索引数据传输到绑定的缓冲区中。`usage` 指示了数据将如何被使用，例如 `gl.STATIC_DRAW` 表示数据不会或几乎不会改变。

4. **顶点属性指针设置**:
   - `gl.vertexAttribPointer(index, size, type, normalize, stride, offset)`：指定顶点属性数组的布局。例如，定义了每个顶点的位置、法线或纹理坐标的数据如何从缓冲区中读取。

5. **启用顶点属性数组**:
   - `gl.enableVertexAttribArray(index)`：启用之前通过 `vertexAttribPointer` 定义的顶点属性。

6. **使用着色器程序**:
   - `gl.useProgram(program)`：指定要使用的着色器程序。

7. **设置Uniform变量**:
   - 如 `gl.uniformMatrix4fv(location, transpose, value)` 用于设置矩阵Uniform变量。
   - `gl.uniform1f(location, value)`、`gl.uniform3fv(location, value)` 等用于设置其他类型的Uniform变量。

8. **激活纹理单元**:
   - `gl.activeTexture(texture)`：激活特定的纹理单元。

9. **绑定纹理**:
   - `gl.bindTexture(target, texture)`：将纹理绑定到指定的纹理目标（如 `gl.TEXTURE_2D`）。

10. **绘制命令**:
    - `gl.drawElements(mode, count, type, offset)`：执行绘制命令，根据索引缓冲区中的索引来绘制顶点。

这些函数和方法共同构成了用于在WebGL中渲染3D网格的基本框架。通过正确地使用这些API，开发者可以在网页上创建复杂的3D场景和图形效果。

-----

如何使用缓冲区？

1. 第一步：创建缓冲区对象（gl.createBuffer()）

   `const buffer = gl.createBuffer()`

1. 第二步：绑定缓冲区对象（gl.bindBuffer()）

   >  **Buffer的种类也很多**
   >
   > **「gl.ARRAY_BUFFER」** 还有 **「gl.ELEMENT_ARRAY_BUFFER」** 这两个有什么区别呢？？？ 什么时候使用呢
   >
   > 就是因为buffer 的类型很多变， 所以你必须手动绑定，表示当前buffer
   >
   > 我先介绍下 这两个buffer 的区别：
   >
   > 1. `gl.ARRAY_BUFFER`: 包含顶点属性的Buffer，如顶点坐标，纹理坐标数据或顶点颜色数据。
   > 2. `gl.ELEMENT_ARRAY_BUFFER`: 用于元素索引的Buffer。
   > 3. 当使用 **「WebGL 2 context」**
   >    时，可以使用以下值：
   >
   > - `gl.COPY_READ_BUFFER`: 从一个Buffer对象复制到另一个Buffer对象。
   > - `gl.COPY_WRITE_BUFFER`: 从一个Buffer对象复制到另一个Buffer对象。
   > - `gl.TRANSFORM_FEEDBACK_BUFFER`: Buffer for transform feedback operations.
   > - `gl.UNIFORM_BUFFER`: 用于存储统一块的Buffer。
   > - `gl.PIXEL_PACK_BUFFER`: 用于像素传输操作的Buffer。
   > - `gl.PIXEL_UNPACK_BUFFER`: 用于像素传输操作的Buffer。
   >
   >  你只要知道，buffer有很多类型， 因为我们是顶点嘛， 所以绑定的肯定是第一个类型
   >
   > ```javascript
   > const canvas = document.getElementById('canvas');
   > const gl = canvas.getContext('webgl');
   > const buffer = gl.createBuffer();
   > gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
   > ```
   >
   > 

2. 第三步：将数据写入缓冲区对象（gl.bufferData()）

   > ```javascript
   > // 三角形的顶点数据
   >   const vertices = new Float32Array([0.0,0.5,-0.5,-0.5,0.5,-0.5]);
   >  //创建缓冲区对象
   >   const vertexBuffer = gl.createBuffer();
   >   if(!vertexBuffer){
   >       console.log("创建缓冲区对象失败");
   >       return -1;
   >   }
   > 
   >   //将缓冲区对象绑定到目标
   >   gl.bindBuffer(gl.ARRAY_BUFFER,vertexBuffer);
   > 
   >   //向缓冲区对象中写入数据
   >   gl.bufferData(gl.ARRAY_BUFFER,vertices,gl.STATIC_DRAW);
   > 
   > ```
   >
   > 第三个参数表示什么呢？？？ ， 表示的向缓冲区对象写入一次数据，但是需要绘制很多次

3. 第四步：将缓冲区对象分配给一个attribute变量（）

   > **「gl.vertexAttribPointer」**
   >
   > 1. 第一个参数的话其实 指定要修改的顶点属性的索引， 而这个索引 就是我们通过上面在顶点着色器中去查找到的
   > 2. 第二参数 的表示 指定每个顶点属性的组成数量，必须是1，2，3或4。这是为什么呢 ，因为顶点坐标 默认是（x,y,z,w）如果你选择是1 那么 程序会帮你把剩下的参数补充。 2 3 设置为0 4 设置为 1 。 因为我们这里 其实就是个二维坐标 ， 所以只用到了 x,y 这样的坐标 所以是2
   > 3. 第三个 表示数据类型， 我们这里是浮点型
   > 4. 第四个 表示归一化 ，就是把 非浮点型的数据 归一到 【0,1】 或者 是【-1，1】 区间
   > 5. 第五个 表示相邻两个顶点之间的字节数
   > 6. 最后一个 表示 我们获取的这个顶点索引 是在缓冲区 的何处 开始存储的 ， 我们缓冲区对象 存的都是顶点数据， 所以这里是0
   >
   > ```javascript
   > if (this.mesh.hasVertices) {
   > 			const numComponents = 3;
   > 			const type = gl.FLOAT;
   > 			const normalize = false;
   > 			const stride = 0;
   > 			const offset = 0;
   > 			gl.bindBuffer(gl.ARRAY_BUFFER, this.#vertexBuffer);
   > 			gl.vertexAttribPointer(
   > 				this.shader.program.attribs[this.mesh.verticesName],
   > 				numComponents,
   > 				type,
   > 				normalize,
   > 				stride,
   > 				offset);
   > 			gl.enableVertexAttribArray(
   > 				this.shader.program.attribs[this.mesh.verticesName]);
   > ```
   >
   > ```javascript
   > void gl.vertexAttribPointer(index, size, type, normalized, stride, offset);
   > ```
   >
   > ①index:指定要修改的顶点属性的**索引**
   >
   > ②size:每个顶点的坐标个数，必须是1，2，3，4
   >
   > ③type:指定数组中每个元素的数据类型可能是（有gl.byte,gl.short)
   >
   > ④normalized:是否进行归一化
   >
   > ⑤stride:以字节为单位指定连续顶点属性开始之间的偏移量（即数组中一行长度）
   >
   > ⑥offset:顶点属性数组中第一部分的字节偏移量

4. 第五步：开启attribute变量（gl.enableVertexAttribArray()）其实就是这么简单的一行代码，表示开启变量 ， 同时建立了 缓冲区对象 和 attribute 之间的链接了
5. **「第一步：创建缓冲区对象（gl.createBuffer()）」**
6. **「第二步：绑定缓冲区对象（gl.bindBuffer()）」**
7. **「第三步：将数据写入缓冲区对象（gl.bufferData()）」**
8. **「第四步：将缓冲区对象分配给一个attribute变量（gl.vertexAttribPointer()）」**
9. **「第五步：开启attribute变量（gl.enableVertexAttribArray()）」**

## 作业一

###WebGL基础

#### WebGL基础概念

1. WebGL需要提供成对的方法。没对方法中一个叫做顶点着色器，一个叫做片段着色器。再着色器语言中使用给GL着色语言（GLSL）

2. 顶点着色器的作用是计算顶点位置（包括了根据三角形的三个顶点进行三角形内部各种属性的插值计算）

3. 对图元进行光栅化处理时需要使用片段着色器。片段着色器的作用是计算出当前绘制图元中每个像素的颜色值。

4. 这些方法对所需的任何数据都需要发送给GPU，着色器获取数据的四种方法

5. `Attribute属性与缓冲`

   - 缓冲是发送到GPU的一系列的二进制数据，这些数据通常包括了（顶点坐标，顶点法向量，顶点的纹理坐标，顶点的颜色值）
   - 属性用于指明怎么从缓冲中或许相应的数据并提供给顶点着色器。（比如可以使用三个4B的浮点型数据3fv去存储一个顶点坐标）（对于一个确切的属性需要指明从那个缓冲中获取数据，获取什么类型的数据（3fv)，起始的偏移值是多少，到下一个位置的字节数是多少）
   - 缓冲不是随意读取的。事实上顶点着色器运行的次数是一个确切的数字。每一次运行属性，都会从指定的缓冲中按照指定规则依次获取下一个值。

6. 全局变量（uniforms)全局变量再着色程序运行之前赋值，再整个着色程序运行中全局有效。

7. 纹理（Textures)纹理：纹理是一个数据序列，可以在着色程序运行中随意读取其中的数据。大多数情况下存放的是图像数据，也可以存放出了颜色数据之外的数据

8. 可变量（varying):可变量是一种顶点着色器给片段着色器传值的方式，依照渲染的的图元是点，线还是三角形，顶点着色器中设置的可变量（Varying)会在片段着色器运行中获取不同的插值。

#### WebGL Hello World

1.  WebGL只关心两件事：裁剪空间中的坐标值和颜色值，也就是说使用WebGL只需要给他提供两个变量即可。其中，需要两个着色器来做这件事。其中，顶点着色器提供裁剪空间的坐标值，片段着色器提供颜色值。
2. 也就是说无论画布有多大，裁剪空间的坐标范围永远是[-1,1]
3. 从简单的顶点着色器开始

```glsl
// 一个属性值，着色器会从缓冲中读取数据
attribute vec4 a_position;

// 所有着色器都有一个main方法
void main()
{
    // gl_Position 是一个顶点着色器主要设置的变量
    gl_Position = a_position;
}
```

如果使用了Javascript代替里顶点着色器语言，相当于在其中做了一下

```javascript
// 定义了一个顶点缓冲
var positionBuffer = [0, 0, 0, 0,
                      0 ,1, 1, 2,
                      2 ,3, 3, 4];
var attributes = {};
val gl_Position;

drawArrays(..., offset, count)
{
    // 每次取数据的间隔
    var stride = 4；
    var size = 4;
    for(let i = 0; i < count; i ++)
        {
            // 从positionBuffer复制接下来4个值给a_Position属性
            const start = offset + i * stride;
            attributes.a_Position = positionBuffer.slice(start, start + size);
            // 运行顶点着色器
            runVertexShader();
            ...
            doSomeThingWith_gl_Position();
        }
}
```

接下来会需要一个片段着色器

```glsl
// 片段着色器没有默认精度， 所以需要手动设置一个精度
// mediup 代表的是 medium precision(中等精度)
precision mediump float;

void main()
{
    // gl_FragColor是一个片段着色器主要设置的变量
    gl_FragColor = vec4(1, 0, 0.5, 1);
    //1 代表红色值，0代表绿色值，0.5代表蓝色值，1代表阿尔法通道值
}
```

现在拥有了顶点着色器和片段着色器，可以开始使用WebGL了

4. 首先需要一个HTML中的canvas(画布)对象

`<canvas id="c"></canva>`

然后就可以使用JavaScript获取画布

```javascript
var canvas = document.querySelector("#c")
```

获取画布后需要创建一个WebGL渲染上下文（WebGLRenderingContext)

```javascript
var gl = canvas.getContext("#webgl")
if(!gl)
    {
        console.log("上下文管理器创建失败")；
    }
```

之后需要编译着色器后提交到GPU中。一种方法是利用JavaScript中创建字符串的方式创建一个GLSL字符串。或者直接将其放在非JavaScript类型的标签中。

```javascript
<script id="vertex-shader-2d" type="notjs">
 
  // 一个属性变量，将会从缓冲中获取数据
  attribute vec4 a_position;
 
  // 所有着色器都有一个main方法
  void main() {
 
    // gl_Position 是一个顶点着色器主要设置的变量
    gl_Position = a_position;
  }
 
</script>
 
<script id="fragment-shader-2d" type="notjs">
 
  // 片段着色器没有默认精度，所以我们需要设置一个精度
  // mediump是一个不错的默认值，代表“medium precision”（中等精度）
  precision mediump float;
 
  void main() {
    // gl_FragColor是一个片段着色器主要设置的变量
    gl_FragColor = vec4(1, 0, 0.5, 1); // 返回“瑞迪施紫色”
  }
 
</script>
```

接下来使用方法创建一个着色器，只需要上传以及写好的GLSL数据，之后编译成为着色器即可。

```javascript
// 创建着色器方法，输入参数：渲染的上下文gl,着色器类型，数据源（着色器源码）
function createShader(gl, type, source)
{
    // 创建着色器对象
    var shader = gl.createShader(type);
    // 提供着色器源码
    gl.shaderSource(shader, source);
    // 编译生成着色器
    gl.compileShader(shader);
    // 判断是否生成成功
    if(gl.getShaderParameter(shader, gl.COMPILE_STATUS) == true)
        {
            return shader;
        }
    else
        {
            console.log(gl.getShaderInfoLog(shader));
            gl.deleterShader(shader);
        }
}
```

之后便可以用这个函数创建两个shader程序（一个顶点着色器程序，一个片段着色器程序）

```javascript
var vertexshadersource = document.querySelector('#vertex-shader-2d').text;
var fragshadersource = document.querySelector('#fragment-shader-2d').text;

var vertexShader = createShader(gl, gl.VERTEX_SHADER， vertexshadersource);
var fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragshadersource);
```

之后可以将两个（顶点着色器和片段着色器连接到一个独有的着色程序上）

```javascript
function createProgram(gl, vertexshader, fragmentshader)
{
    var program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    // boolean变量判断是否创建成功
    var success = gl.getProgramParameter(program. gl.LINK_STATUS);
    if(success)
        {
            return program;
        }
    else
        {
            console.log(gl.getProgramInfoLog(program));
            gl.deleteProgram(program);
        }
}
```

之后就可以调用function生成一个GLSL着色程序

```javascript
var program = createProgram(gl, vertexShader, fragmentShader);
```

现在已经在GPU上成功创建了一个着色程序，我们需要给其提供数据。WebGL的主要任务就是设置好状态并给GLSL着色程序提供所需的数据。在这个例子中GLSL着色程序的唯一输入就是一个属性值Attribute（a_position)。

我们要做的第一件事就是从着色程序中找到这个属性值所在的位置，可以使用以下方法

```javascript
var a_positionAttributeLocation = gl.getAttribLocation(program, "a_position");
```

寻找属性值位置（以及全局变量）的位置应该在初始化完成，而不是在渲染循环中。

属性值从缓冲中获取数据，需要创建一个缓冲

```javascript
var positionbuffer = gl.createBuffer();
```

WebGL可以通过绑定点操控全局的许多数据，可以把绑定点想象为一个WebGL内部的全局变量。首先帮点一个数据源到绑定点（ARRAY_BUFFER)，然后就可以引用绑定点指向该数据源。绑定位置信息的缓冲

```javascript
gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer)
```

现在我们需要通过绑定点向缓冲中存放数据

> 可以理解为：绑定点就是外界与缓冲区交换数据的中介
>
> 是的，您的理解是正确的。在 WebGL 中，绑定点（binding point）可以被看作是在 WebGL 内部用于引用和操作数据的“全局变量”。**绑定点充当了一种中介，使得您可以在 WebGL 的上下文中操控和引用数据**。具体来说：
>
> 1. **创建和绑定缓冲区**：
>    - 首先，您创建了一个缓冲区（`positionBuffer`），这是用于存储数据（如顶点位置信息）的内存区域。
>    - 然后，通过 `gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);` 将这个缓冲区绑定到 `gl.ARRAY_BUFFER` 这个绑定点上。这个操作实际上是告诉 WebGL：“现在我要操作的是 `positionBuffer` 这个缓冲区。”
>
> 2. **绑定点作为数据操作的参照**：
>    - 绑定缓冲区到 `ARRAY_BUFFER` 后，之后的所有对 `ARRAY_BUFFER` 的操作都会作用于当前绑定的缓冲区，即 `positionBuffer`。
>    - 例如，当您使用函数像 `gl.bufferData()` 或 `gl.bufferSubData()` 来向缓冲区传输数据时，这些数据会被存储到当前绑定到 `ARRAY_BUFFER` 的缓冲区中（在这个例子中，就是 `positionBuffer`）。
>
> 3. **绑定点简化了数据管理**：
>    - 由于 WebGL 是一个基于状态的绘图系统，绑定点机制简化了数据和资源的管理。您不需要在每次操作时都指定具体的缓冲区，而只需引用相应的绑定点。
>    - 这样的设计使得您可以在不同的渲染阶段方便地切换和管理不同的数据资源。
>
> 总之，绑定点在 WebGL 中起到了连接您的程序和 WebGL 内部数据存储（如缓冲区）之间的桥梁作用，使得数据的上传、修改和引用变得更加高效和方便。

现在我们需要通过绑定点向缓冲区中存放数据

```javascript
// 三个二维点坐标
var positions = [
    0, 0,
    0, 0.2,
    0.7, 0
];

gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);
```

这里完成了一系列事情

- 第一件事就是有了一个JavaScript序列数据，他代表的是顶点的坐标
- new Float32Array创建了一个32为浮点型数据序列，并将其从positions中复制到该序列中
- gl.bufferData复制这些数据到已经创建好的Buffer(positionBuffer)上（这里ARRAY_BUFFER就充当了一个外界和GPU内部positionBuffer交换数据的中介）
- 最后一个参数gl.STATIC_DRAW提示WebGL我们会如何操作这些数据，STATIC提示webGL我们不会经常改变这些数据

**以上都是初始化渲染程序以及向渲染程序中传递数据的初始化代码，只会在页面加载时运行依次，接下来的便是渲染代码，这些代码会在我们每次需要渲染以及绘制时执行**

#### 渲染

在绘制之前我们应该调整画布（canvas)的尺寸以匹配它的显示尺寸。

因为画布就像图片，一般拥有两个尺寸。一个是其拥有的实际的像素数量，一个是它显示的带线啊哦。CSS可以决定画布显示的大小。

在渲染中。

我们需要告诉web GL怎样把提供的gl_position即裁剪空间的坐标如何对应带画布的像素坐标（画布的像素坐标就可以叫做window coordinates,或者是屏幕坐标screen coordinates),需要进行的变换的矩阵即视口变换矩阵，即viewportmatrix,为了实现这个目的，我们只需要调用gl.viewport方法传递画布当前画布的当前尺寸

```javascript
gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);
```

这样就可以告诉WebGL裁剪空间的（-1，1）对应到x轴的屏幕空间的范围为（0，gl.canvas.width),对应到y轴的屏幕空间的范围为（0，gl.canva.height)。

我们可以使用（0，0，0，0）清空画布，分别对应着rgb,aplha值，使用全零清空画布后可以让画布 变得透明

```javascript
gl.clearColor(0, 0, 0, 0);
gl.clear(gl.COLOR_BUFFER_BIT);
```

之后需要告诉WebGL运行哪一个着色程序

```javascript
gl.useProgram(program);
// 告诉它用我们之前写好的着色程序，一个着色对
```

接下来我们需要告诉WebGL，，怎么从之前准备的缓冲中，，，从缓冲中获取数据，，获取的数据提供给着色器中的属性。

首先需要启用对应的属性

```javascript
gl.enableVertexAttribArray(positionAttributeLocation);
```

然后指定从缓冲中读取数据的方式

```javascript
// 将绑定点绑定到缓冲数据
gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);

// 告诉属性怎么从positionBuffer(ARRAY_BUFFER)中读取数据
var size = 2; // 每次迭代提取两个单位数据
var type = gl.FLOAT; // 每个单位的数据类型是32位浮点型
var normalize = false; // 不需要将数据归一化
var stride = 0; // 每次迭代运行运动多少内存到下一个数据开始点
var offset = 0; // 从缓冲起始位置开始读取
gl.vertexAttribPointer(
positionAttributeLocation, size, type, normalize, stride, offset);
```

**一个隐藏信息是gl.vertexAttribPointer是将属性绑定到当前的ARRAY_BUFFER,换句话说就是属性绑定到了positionBuffer上，这也意味着现在利用绑定点随意将ARRAY_BUFFER绑定到其他数据上后，该属性依然会从positionBuffer上读取数据。**

```javascript
"use strict";


main()

function createshader(gl, type, source)
{
    var shader = gl.createshader(type);
    gl.shaderSource(shader, source);
    gl.compileShader(shader);
    var success = gl.getShaderParameter(shader, gl.COMPILE_STATUS);
    if(success)
    {
        return shader;
    }

    console.log(gl.getShaderInfoLog(shader));
    gl.deleteShader(shader);
}

function createProgram(gl, vertexShader, fragmentShader)
{
    var program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    var success = gl.getProgramParameter(program, gl.LINK_STATUS);
    if(success)
    {
        return program;
    }
    console.log(gl.getProgramInfoLog(program));
    gl.deleteProgram(program);
}

function main()
{
    // 获得一个webgl上下文
    var canvas = document.querySelector("#c");
    var gl = canvas.getContext("webgl");;
    if(!gl){
        return ;
    }


    // 获得顶点着色器和片段着色器的源码
    var vertesshadersource = document.querySelector("#vertex-shader-2d").textContent;
    var fragmentshadersource = document.querySelector("#fragment-shader-2d").textContent;

    // create glsl shader, upload the glsl source, compile the shader
    var vertexShader = createShader(gl, gl.VERTEX_SHADER, vertesshadersource);
    var fragmentShader = createShader(gl, gl.FRAGMEN_SHADER, fragmentshadersource);

    // link the shader into a shader program
    var program = createProgram(gl, vertexShader, fragmentShader);

    // look up where the vertex data needs to go
    var positionAttributeLocation = gl.getAttribLocation(program, "a_position");

    // create a buffer to put three 2D clip space points in it
    var positionBuffer = gl.createBuffer();

    // bind it to ARRAY_BUFFER(即ARRAY_BUFFER是数据和GPU空间的媒介)
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);

    // create a new array of positions
    var positions = [
        0 , 0,
        1 , 2,
        3 , 4 ]

    // 把数据放到BUFFER中
    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);

    // 以下的代码是渲染时需要运行的代码
    webglUtils.resizeCanvasToDisplaySize(gl.canvas);

    // Tell webgl how to convert from clip space to window space
    gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);

    // clear the canvas
    gl.clearColor(0, 0, 0, 0);
    gl.clear(gl.COLOR_BUFFER_BIT); // opaque

    // tell the webgl to use the shader program 
    gl.useProgram(program);

    // 使用指向buffer的位置属性（启用指针)
    gl.enableVertexAttribArray(positionAttributeLocation);

    // tell the attribute how to get data out of positionBuffer
    var size = 2;          // 2 components per iteration
    var type = gl.FLOAT;   // the data is 32bit floats
    var normalize = false; // don't normalize the data
    var stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position
    var offset = 0;        // start at the beginning of the buffer
    gl.vertexAttribPointer(positionAttributeLocation, size, type, normalize, stride, offset);

    // draw the triangle
    var primitiveType = gl.TRIANGES;
    var offset = 0;
    var count = 3;
    gl.drawArrays(primitiveType, offset, count);
}
```

着色语言GLSL时如下

```glsl
<canvas id="c"></canvas>
  <script  id="vertex-shader-2d" type="notjs">

  // an attribute will receive data from a buffer
  attribute vec4 a_position;

  // all shaders have a main function
  void main() {

    // gl_Position is a special variable a vertex shader
    // is responsible for setting
    gl_Position = a_position;
  }

</script>
<script  id="fragment-shader-2d" type="notjs">

  // fragment shaders don't have a default precision so we need
  // to pick one. mediump is a good default
  precision mediump float;

  void main() {
    // gl_FragColor is a special variable a fragment shader
    // is responsible for setting
    gl_FragColor = vec4(0.8, 0.2, 0.9, 1); // return redish-purple
  }
```

其中只有一个变量需要传递，就是编译后program中规定的attribute变量a_positions,在GLSL中规定的Attribute时需要WebGL传递给着色器程序的变量。通过接口`gl.vertexAttribPointer`函数进行数据的传递

----

#### WebGL工作原理

WebGL在GPU上工作基本上分为两个部分，第一部分是将顶点（或数据流）转换位裁剪空间中的坐标，第二部分是基于第一部分的结果取绘制像素点

当调用

```javascript
var primitiveType = gl.TRIANGLES;
var offset = 0;
var count = 9;
gl.drawArrays(primitiveType, offset, count);
```

这里count = 9所以会处理九次顶点信息

![img](https://webglfundamentals.org/webgl/lessons/resources/vertex-shader-anim.gif)

左侧是传入Buffer的数据信息，（vertex shader）是写入GL的一个方法，每个顶点都会被vertex shader去调用一次，在这个方法中做一些在vertex shader中定义的方法之后，设置了一个特殊的变量`gl_Position`保存起来，这个变量就是将顶点从局部坐标系转换为裁剪空间的坐标值（这里定义的全局变量uniform u_matrix对应的就是MVP变换矩阵），GPU会接收gl_Position并将其保存。

假设你正在画三角形。顶点着色器每完成三次顶点处理后，WebGL就会用三个顶点画出三角形。也就是所谓的，计算出三个顶点后，就会光栅化这个三角形。光栅化就是所谓的用像素将这个三角形画出来。**对于每一个像素，片元着色器都会询问你这个像素需要使用什么颜色进行上色，你可以通过一个`gl_FragColor`**设置当前片元（像素）的颜色。

在上面的片元着色器中，你每次对于gl_FragColor的返回值都是一个固定的向量（颜色），如果你需要多变的颜色，就需要将一些信息（必须顶点的纹理坐标，顶点法线）在片元着色器中进行插值，以确定当前像素的颜色。对于需要从顶点着色器传入片元着色器的变量，一般使用Varying进行申明。

比如将顶点着色器计算出的裁剪空间坐标从顶点着色器传入片元着色器中。

在顶点着色器中定义varying变量传给片元着色器

```glsl
varying vec4 v_color;
...
void main(void)
{
    gl_position = vec4((u_matrix) * vec3(a_position, 1).xy, 0, 1);
    
    .....
        
     v_color = gl_position * 0.5 + 0.5;
}
```

那么也需要在片元着色器定义同样的Varying变量

```glsl
#ifdef GL_ES
precision mediump float;
#endif

varying vec4 v_color;

// 在main函数中使用这个颜色值
void main()
{
    gl_fragcolor = v_color;
}
```

我们只计算了三个顶点，调用了三次顶点着色器，按理来说应该也只有三个点的颜色，为什么可以可以绘制出三角形呢？

WebGL先在顶点着色器中计算出了三个顶点的颜色。在光栅化三角形时就会根据三角形的三个顶点信息进行相应的重心坐标插值计算。**也就是说，每一个像素在调用片元着色器时，可变的量Varying都是得到插值之后的值！！！**

我们从上述的三个顶点开始分析

| 顶点 |      |
| ---- | ---- |
| 0    | -100 |
| 150  | 125  |
| -175 | 100  |

我们给三个局部的空间的点进行ModelViewTransform, ProjectionTransform后，将坐标转换到了裁剪空间坐标中，得到在裁剪空间中的值为（也就是写入了gl_position)的信息

| 顶点   |        |
| ------ | ------ |
| 0      | 0.660  |
| 0.750  | -0.830 |
| -0.875 | -0.660 |

在顶点着色器中，进行顶点颜色的计算后写入Varying的变量v_color中的值为

| 写入 v_color 的值 |       |      |
| ----------------- | ----- | ---- |
| 0.5000            | 0.830 | 0.5  |
| 0.8750            | 0.086 | 0.5  |
| 0.0625            | 0.170 | 0.5  |

利用计算的顶点信息，可以在片元着色器中对每个像素的颜色进行插值

![片元着色器中的插值](D:\BaiduNetdiskWorkspace\games202\片元着色器中的插值.png)

综上，想给片段着色器传值，我们可以先把值传递给顶点着色器进行计算后，在传给片段着色器进行着色。

#### buffer和attribute的代码是干什么的？

缓冲操作是在GPU上获取顶点和其他顶点数据的一种方式。gl.createBuffer创建了一个缓冲；gl.bindBuffer是设置了缓冲为当前指定的缓冲，gl.bufferData将数据拷贝到了缓冲中，这些操作一般都会在初始化中进行。

**一旦数据送入了缓冲中，还需要WebGL如何从缓冲中提取数据，并如何传给顶点着色器！！！！**

第一步：需要获取WebGL给属性分配的地址（获得一个指针）这一步通常也会在初始化时完成

```javascript
var positionLocation = gl.getAttribLocation(program, "a_position")
var colorLocation = gl.getAttribLocation(program, "a_color");
```

第二步：一旦知道了属性的地址，还要将这个指针启用（也就说告诉WebGL我们想从缓冲中获取这个数据）

```javascript
gl.enableVertexAttribArray(positionLocation)
gl.enableVertexAttribArray(colorLocation)
```

我们还需要将创建好的缓冲绑定到一个绑定点上，绑定点充当的角色就是将外部数据送到创建好的缓冲区中

```javascript
gl.bindBuffer(gl.ARRAY_BUFFER, positon_buffer)
gl.bindBuffer(gl.ARRAY_BUFFER, colot_buffer)
```

之后需要告诉WebGL去指针对应的地址去获得对应的某些数据

```javascript
gl.vertexAttribPointer(
	location(即之前创建的一个Location指针）,
	numComponents,
    typeofData,
    normalizeFlag,
    stride,
    offset)
```

每个数据都使用单独的一个缓冲时，stride和offset都是0，但是如果你想要多个数据贡献一个缓冲，那么就需要自己计算stride和offset的值。

#### WebGL着色器和GLSL

#### 顶点着色器

一个顶点着色器的主要工作就是生成裁剪空间的坐标值。通常是以下形式

```glsl
void main()
{
    gl_position = MVPTRAMSFORM_TO_GET_CLIP_COORDINATES;
}
```

每个顶点都需要调用一次顶点着色器，每次调用都需要设置一个特殊的全局变量值`gl_position`,该变量就是经过裁剪的空间坐标值

其中

**对于顶点着色器所需要的数据，可以通过以下方式获得！！！！**

- Attributes属性：从缓冲中获取的数据，也就是每次获取的数据都不同（比如获取的顶点的坐标，顶点的法线）
- Uniforms全局变量：即在一次绘制中，所有的顶点都会进行贡献的值，对所有顶点都是一视同仁的
- Textures纹理：从像素或是纹理元素中获取的值，相当于获取每个顶点的纹理坐标，每次获取的值也都是不一样的

##### 对于Attributes怎么获取呢？

从缓冲中获取

首先需要创建缓冲

```javascript
var buffer = gl.createBuffer();
```

之后需要绑定缓冲到缓冲点（ARRAY_BUFFER)

```javascript
gl.bindBuffer(ARRAY_BUFFER, buffer);
```

绑定之后就可以向缓冲点写入数据，类比于向缓冲中写入数据

```javascript
gl.bufferData(gl.ARRAY_BUFFER, somedata, gl.STATIC_DRAW)
```

获取缓冲的首地址以找到属性所在的地址

```javascript
var positonLocation = gl.getAttribLocation(ShaderProgra, "a_position")
```

渲染时告诉WebGL如何从缓冲中取出数据并送入顶点着色器中

```javascript
// 启用地址
gl.enableVertexAttribArray(positionLocation);

// 从缓冲中读取数据送入顶点着色器程序中
var numComponents = 3;
var type = gl.FLOAT;
var normalized = false;
var stride = 0;
var offset - 0;
gl.vertexAttribPointer(positionLocation, numComponents, type, normalized, stride, offset)
```

在顶点着色器中就可以使用传入的顶点数据进行运算了。下列示例了在vertexshader中进行运算的简单情形

```glsl
attributes vec4 a_position;

void main()
{
    gl_position = a_position;
}
```

##### 对于uniforms变量如何获取呢

全局变量在每一次顶点计算中都是完全相同的，在下面一个简单例子中，用全局变量给顶点着色器添加了一个偏移量

```glsl
attribute vec4 a_position;
uniform vec4 u_offset;

void main()
{
    gl_positon = a_position + u_offset;
}
```

如何对全局变量进行初始化呢？

首先在初始化时找到全局变量的地址

```javascript
var uniformoffsetLocation = gl.getUniformLocation(someProgram, "u_offset")
```

在渲染绘制之前给全局变量进行赋值

```javascript
gl.uniform4fv(uniformoffsetLocation, [1, 0, 0, 0])
```

要注意全局变量是对于单个着色程序的，如果有多个着色程序有同名全局变量，徐娅萍找到每个全局变量并设置自己的值。**我们在调用gl.uniform???时，只是设置了当前程序的全局变量**

##### 纹理怎么设置呢

同片段着色器中的Texturesw纹理

#### 片段着色器

一个片段着色器的工作是为当前光栅化的像素提供颜色值，通常时以下的形式对当前的像素进行赋值

```glsl
precision mediump float;

void main()
{
    gl_FragColor = Do_Math_To_Get_Color;
}
```

每个像素都会调用一次片段着色器，每次对于像素的颜色都会从gl_FragColor中获取。

片段着色器的数据，可以从以下途径中获取

- Uniforms全局变量（对于每个像素的着色都是完全相同的）
- Textures纹理
- Varying(从顶点着色器获得并插值到当前像素)

##### Textures纹理怎么获取？

在着色器中获取纹理信息，可以先创建一个**sampler2D类型的全局变量**，然后使用

**texture2D方法，从纹理中获取信息**，如下

```glsl
precision mediump float;

uniform sampler2D u_texture;

void main()
{
    // 需要获取纹理值的坐标，纹理坐标
    vec2 texcoords = vec2(0.5, 0.5);
    gl_FragColor = texture2D(u_texture, texcoords);
}
```

那么如何向uniform sampler2D 变量中写入全局变量的数据呢？

这就是一件很复杂的事，之后在开一篇。但至少要创建并给这个纹理数据填充数据。例如

```javascript
// 创建一个纹理
var tex = gl.createTexture();
// 绑定纹理到一个纹理的绑定点
gl.bindTexture(gl.TEXTURE2D, tex);

var level = 0;
var width = 2;
var height = 1;
var data = new int3Array([
    255, 0, 0, 255,
    0, 255, 0, 255,
])

gl.texImage(gl.TEXTURE_2D, level, gl.RGBA, wigth, height, 0, gl.RGBA, gl.UNSIGNED_BYTE, data);
// 设置纹理的参数
gl.texParameteri(gl.TEXTURE2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
```

在初始化全局变量时找到纹理的地址

```javascript
var someSampleLocation = gl.getUniformLocation(someProgram, 'u_texture');
```

在渲染时，WebGL要求纹理必须绑定到一个纹理单元上

```javascript
// 指定纹理单元
var unit = 5;
gl.activeTexture(gl.TEXTURE0 + unit);
gl.bindTexture(gl.TEXTURE2D, tex);
```

然后告诉着色器你要使用的纹理在那个纹理单元里

```javascript
gl.uniform1i(someSampleLocation, unit);
```

##### Varying变量

Varying变量就是之前说过的，通过顶点着色器传给片元着色器后，可以进行插值计算

#### WebGL的三维纹理

在WebGL中如何使用纹理？

首先需要调整着色器以便使用纹理，在顶点着色器中，我们需要修改，需要在顶点着色器中定义纹理坐标，以便直接传入片段着色器中

```glsl
// 顶点着色器
attribute vec4 a_position；
// 从缓冲中传入的纹理坐标
attribute vec2 a_textcoord;

uniform mat4 u_matrix;

varying vec2 v_texcoord;

void main()
{
    // 在顶点着色器中的特殊全局变量
    gl_position = u_matrix * a_position;
    
    // 传递纹理坐标到片段着色器中
    v_texcoord = a_textCoord;
}
```

注意！在顶点着色器中传入的仅仅是纹理坐标，并不是真正的纹理。

在片段着色器中申明一个sampler2D类型的全局变量，可以在纹理上查询插值的纹理坐标获得纹理（可以让我们引用一个纹理，然后使用从顶点着色器传入的纹理坐标调用texture2D方法，在纹理上找到对应的颜色）

以下是片段着色器的修改

```glsl
precison mediump float;

// 从顶点着色器传入的纹理坐标
varying vec2 v_texcoords;

// 纹理
uniform sampler2D u_texture;

void main()
{
    // 在片段着色器中的重要变量，当前像素的颜色
    // 查询纹理获得纹理颜色
    gl_FragColor = texture2D(u_texture, v_texcoords);
}
```

我们需要在JavaScript中设置纹理坐标

```javascript
// 在程序中找到顶点坐标的属性
var positionLocation = gl.getAttribLocation(program, 'a_position');
var texcoordLocation = gl.getAttribLocation(program, 'a_texcoord');
...

// 为纹理坐标创建一个缓冲以便顶点着色器从缓冲中取数据
var buffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
// 启用顶点指针
gl.enableVertexAttribArray(texcoordLocation);

// 设置读写格式以便webGL想顶点着色器中传送数据
gl.vertexAttribPointer(texcoordLocation, 2, gl.FLOAT, false, 0, 0);

// 设置纹理坐标
setTexcoords(gl);


// 如所见，我们会把图像映射到F的每个矩形面上
function setTexcoord(gl){
    // 向缓冲中传递纹理坐标
    gl.bufferData(
        gl.ARRAY_BUFFER,
        new Float32Array([
            // 正面左竖
            0 , 0,
            0 , 1,
            1 , 0,
            0 , 1,
            1 , 1,
            1 , 0,

            // 正面上横
            0 , 0,
            0 , 1,
            1 , 0,
            0 , 1,
            1 , 1,
            1 , 0,
        ])
        gl.STATIC_DRAW
    )
}
```

有了纹理坐标还不够，还需要一个纹理，纹理就是一张图片，每个像素都会从纹理坐标查询图片中对应的纹素以填充这个像素

加载图片的过程是异步的，我们请求图片资源后浏览器需要一段时间去下载。通常有两种常见的处理方法，一种是等纹理下载完成后开始绘制，另一种是在图像加载前使用生成的纹理，这种方式常常可以立即启动渲染，一旦图像下载完成就拷贝到纹理。我们将使用下面提供的方法

```javascript
// 创建一个纹理
var texture = gl.createTexture();
gl.bindTexture(gl.TEXTURE_2D, texture);

// 先使用一个 1*1 蓝色像素填充纹理
gl.textImage2D(gl.TEXTURE_2D, 0, gl.RGBA, 1, 1, 0, gl.RGBA, gl.UNSIGNED_BYTE, new Uint8Array([0, 0, 255, 255]));

// 异步加载图像,创建一个图像对象
var image = new Image();
// 设置图像的源头
image.src = "resources/f-texture.png";
// 等待图片加载完成后执行回调函数
image.addEventListener("load", function(){
    // 现在图像加载完成后拷贝到纹理中
    gl.bindTexture(gl.TEXTURE_2D, texture);
    gl.texImage2D(gl.Texture_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image);
    // 生成mipmap
    gl.generateMipmap(gl.TEXTURE_2D);
})
```

如果我只想使用一部分图像覆盖模型的一部分，纹理就是通过使用纹理坐标进行引用的，纹理坐标uv的范围都是[0 , 1]
这和传统的像素坐标是完全不同的，在gl中创建纹理时，会自动的转化为可以查询的纹理坐标的格式

如果纹理坐标超出了[0 , 1]的范围会怎么样？WebGL默认会重复纹理，即比如0.0到1.0的纹理会被用于重复1.0到2.0的纹理，以此类推

我们可以在通过设置纹理参数以改变纹理的拓展方式

```javascript
"use strict";

var zDepth = 50;

function main() {
  // Get A WebGL context
  var canvas = document.querySelector("#canvas");
  var gl = canvas.getContext("webgl", {antialias: false});
  if (!gl) {
    return;
  }

  // setup GLSL progra
  // 根据着色器源码创建一个着色器程序
  var program = webglUtils.createProgramFromScripts(gl, ["vertex-shader-3d", "fragment-shader-3d"]);

  // look up where the vertex data needs to go.
  // 看看着色器中的Attribute变量的位置
  var positionLocation = gl.getAttribLocation(program, "a_position");
  var texcoordLocation = gl.getAttribLocation(program, "a_texcoord");

  // lookup uniforms
  // 查询全局变量的位置
  var matrixLocation = gl.getUniformLocation(program, "u_matrix");
  var textureLocation = gl.getUniformLocation(program, "u_texture");

  // Create a buffer for positions
  var positionBuffer = gl.createBuffer();
  // 创建一个缓冲以向缓冲中写入数据
  // Bind it to ARRAY_BUFFER (think of it as ARRAY_BUFFER = positionBuffer)
  // 绑定缓冲
  gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);
  // Put the positions in the buffer
  // 把顶点数据放在缓冲中
  setGeometry(gl);

  // provide texture coordinates for the rectangle.
  // 创建一个纹理坐标的缓冲
  var texcoordBuffer = gl.createBuffer();
  // 绑定缓冲
  gl.bindBuffer(gl.ARRAY_BUFFER, texcoordBuffer);
  // Set Texcoords.
  // 向缓冲中写入顶点数据
  setTexcoords(gl);

  // Create a texture.
  // 创建一个纹理
  var texture = gl.createTexture();
  gl.bindTexture(gl.TEXTURE_2D, texture);
  // Fill the texture with a 1x1 blue pixel.
  // 先用一个 普通图像填充纹理，以等待图片的下载完成
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, 1, 1, 0, gl.RGBA, gl.UNSIGNED_BYTE,
                new Uint8Array([0, 0, 255, 255]));
  // Asynchronously load an image
  // 异步加载图像
  var image = new Image();
  image.src = "https://webglfundamentals.org/webgl/resources/f-texture.png";
  image.addEventListener('load', function() {
    // Now that the image has loaded make copy it to the texture.
    gl.bindTexture(gl.TEXTURE_2D, texture);
    gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA,gl.UNSIGNED_BYTE, image);

    // 检查图像是否是2的幂次方，如果是，则生成mipmap
    // Check if the image is a power of 2 in both dimensions.
    if (isPowerOf2(image.width) && isPowerOf2(image.height)) {
       // Yes, it's a power of 2. Generate mips.
       gl.generateMipmap(gl.TEXTURE_2D);
    } else {
       // No, it's not a power of 2. Turn of mips and set wrapping to clamp to edge
       // 不是2的幂次方，关闭mipmap，设置wrap模式为CLAMP_TO_EDGE
       gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
       gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
       gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
    }
    // 画出图形
    drawScene();
  });

  ---- 以上都是一些初始化-----

  var wrapS = gl.REPEAT;
  var wrapT = gl.REPEAT;

  document.querySelector("#wrap_s0").addEventListener('change', function() { wrapS = gl.REPEAT;          drawScene(); });  // eslint-disable-line
  document.querySelector("#wrap_s1").addEventListener('change', function() { wrapS = gl.CLAMP_TO_EDGE;   drawScene(); });  // eslint-disable-line
  document.querySelector("#wrap_s2").addEventListener('change', function() { wrapS = gl.MIRRORED_REPEAT; drawScene(); });  // eslint-disable-line
  document.querySelector("#wrap_t0").addEventListener('change', function() { wrapT = gl.REPEAT;          drawScene(); });  // eslint-disable-line
  document.querySelector("#wrap_t1").addEventListener('change', function() { wrapT = gl.CLAMP_TO_EDGE;   drawScene(); });  // eslint-disable-line
  document.querySelector("#wrap_t2").addEventListener('change', function() { wrapT = gl.MIRRORED_REPEAT; drawScene(); });  // eslint-disable-line

  function isPowerOf2(value) {
    return (value & (value - 1)) === 0;
  }

  function radToDeg(r) {
    return r * 180 / Math.PI;
  }

  function degToRad(d) {
    return d * Math.PI / 180;
  }

  var fieldOfViewRadians = degToRad(60);

  drawScene();

  window.addEventListener('resize', drawScene);

  // Draw the scene.
  // 画出图像。会在图像加载完成后调用drawScene函数
  function drawScene() {
    webglUtils.resizeCanvasToDisplaySize(gl.canvas);

    // Tell WebGL how to convert from clip space to pixels
    // 设置视口
    gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);

    // Clear the framebuffer texture.
    // 清空Buffers
    gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

    gl.enable(gl.CULL_FACE);
    gl.enable(gl.DEPTH_TEST);

    // Compute the matrix
    var scaleFactor = 2.5;
    var tsize = 80 * scaleFactor;
    var x = gl.canvas.clientWidth / 2 - tsize / 2;
    var y = gl.canvas.clientHeight - tsize - 60;
    gridContainer.style.left = (x - 50 * scaleFactor) + 'px';
    gridContainer.style.top  = (y - 50 * scaleFactor) + 'px';
    gridContainer.style.width  = (scaleFactor * 400) + 'px';
    gridContainer.style.height = (scaleFactor * 300) + 'px';

    // Tell it to use our program (pair of shaders)
    gl.useProgram(program);

    // Turn on the position attribute
    // 启用顶点坐标缓冲的指针
    gl.enableVertexAttribArray(positionLocation);

    // Bind the position buffer.
    // 将顶点缓冲指针和顶点缓冲绑定
    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);

    // Tell the position attribute how to get data out of positionBuffer (ARRAY_BUFFER)
    var size = 3;          // 3 components per iteration
    var type = gl.FLOAT;   // the data is 32bit floats
    var normalize = false; // don't normalize the data
    var stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position
    var offset = 0;        // start at the beginning of the buffer
    // 告诉WebGL如何从缓冲中取出数据送入顶点着色器
    gl.vertexAttribPointer(
        positionLocation, size, type, normalize, stride, offset);

    // Turn on the texcoord attribute
    // 启用纹理坐标缓冲的指针
    gl.enableVertexAttribArray(texcoordLocation);

    // bind the texcoord buffer.
    // 将纹理坐标缓冲与纹理坐标缓冲指针绑定
    gl.bindBuffer(gl.ARRAY_BUFFER, texcoordBuffer);

    // Tell the texcoord attribute how to get data out of texcoordBuffer (ARRAY_BUFFER)
    var size = 2;          // 2 components per iteration
    var type = gl.FLOAT;   // the data is 32bit floats
    var normalize = false; // don't normalize the data
    var stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position
    var offset = 0;        // start at the beginning of the buffer
    // 告诉WebGL如何从纹理坐标缓冲中取出数据以送入顶点着色器中
    gl.vertexAttribPointer(
        texcoordLocation, size, type, normalize, stride, offset);

    // Compute the projection matrix
    var projectionMatrix =
        m4.orthographic(0, gl.canvas.clientWidth, gl.canvas.clientHeight, 0, -1, 1);

    // 绑定已经设置好的纹理（初始化时设置的纹理）
    gl.bindTexture(gl.TEXTURE_2D, texture);
    // 设置纹理参数
    // 第一个参数就是指绑定好的纹理
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, wrapS);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, wrapT);

    var matrix = m4.translate(projectionMatrix, x, y, 0);
    matrix = m4.scale(matrix, tsize, tsize, 1);
    matrix = m4.translate(matrix, 0.5, 0.5, 0);

    // Set the matrix.
    gl.uniformMatrix4fv(matrixLocation, false, matrix);

    // Tell the shader to use texture unit 0 for u_texture
    gl.uniform1i(textureLocation, 0);

    // Draw the geometry.
    gl.drawArrays(gl.TRIANGLES, 0, 1 * 6);
  }
}

// Fill the buffer with the values that define a plane.
function setGeometry(gl) {
  var positions = new Float32Array(
    [
      -0.5,  0.5,  0.5,
       0.5,  0.5,  0.5,
      -0.5, -0.5,  0.5,
      -0.5, -0.5,  0.5,
       0.5,  0.5,  0.5,
       0.5, -0.5,  0.5,
    ]);
  gl.bufferData(gl.ARRAY_BUFFER, positions, gl.STATIC_DRAW);
}

// Fill the buffer with texture coordinates for a plane.
function setTexcoords(gl) {
  gl.bufferData(
      gl.ARRAY_BUFFER,
      new Float32Array(
        [
          -3, -1,
           2, -1,
          -3,  4,
          -3,  4,
           2, -1,
           2,  4,
        ]),
      gl.STATIC_DRAW);
}

main();

```

先解释什么是`gl.generateMipmap`,假设我们有一个16\*16像素的纹理，假设我们要把绘制在屏幕2*2的像素上，那么这四个像素应该是什么颜色呢？这里有256个像素可以选择，如果在PhotoShop中将16\*16的图像缩放到2\*2,它会将每个角8\*8的像素的平均值赋值给这四个像素。不幸的是绘制64个像素再求平均再GPU中是很慢很慢的。假设你有一个2048\*2048像素的纹理想要绘制成2\*2的像素，这需要的运算也是很大的。
事实上，GPU会使用纹理贴图（MipMap），纹理贴图是一个逐渐缩小的图像的集合，每一个都是前一个的几分之几，会提前计算好并保存下来。
一个16\*16的贴图会看起来像这样
![img](https://webglfundamentals.org/webgl/lessons/resources/mipmap-low-res-enlarged.png)
每一个纹理子图都是前一个的双线性插值，这就是`gl.generateMipmap`的作用，它根据原始图像创建所有的缩小级别
现在如果你想将16\*16的像素的纹理绘制到屏幕2\*2的像素上，WebGL会自动从创建的MipMap中查询到合适的纹理以将其填入。

你可以为纹理选择不同的贴图筛选条件来控制WebGL的插值，一共有以下六种模式
- NEAREST：最邻近插值，选择距离纹理坐标最近的像素
- LINEAR = 从最大的贴图中选择4个像素然后混合
- NEAREST_MIPMAP_NEAREST = 选择最合适的贴图，然后从上面找到一个像素
- LINEAR_MIPMAP_NEAREST = 选择最合适的贴图，然后取出 4 个像素进行混合
- NEAREST_MIPMAP_LINEAR = 选择最合适的两个贴图，从每个上面选择 1 个像素然后混合
- LINEAR_MIPMAP_LINEAR = 选择最合适的两个贴图，从每个上选择 4 个像素然后混合

你可以通过这两个例子看到贴图的重要性，第一个显示的是使用 NEAREST 或 LINEAR， 只从最大的体贴图上选择像素，当物体运动时就会出现抖动。由于每个像素都从最大的图上选择， 随着位置和大小的改变，可能会在不同的时间选择不同的像素，从而出现抖动。
https://webglfundamentals.org/webgl/webgl-3d-textures-mips.html


在WebGL中，对于纹理是有限制的，在WebGL中限制了纹理的维度必须是2的整数次幂。对于一张图片的像素是320\*240，都不是2的次幂，用这张图片作为纹理时，就会显示纹理失败。所以在着色器中使用`texture2D`被调用时，由于纹理设置不正确，就会使用（0，0，0，1）的全黑色。如果此时打开JavaScript控制台，就会法线以下的报错信息
```
WebGL: INVALID_OPERATION: generateMipmap: level 0 not power of 2
   or not all the same size
WebGL: drawArrays: texture bound to texture unit 0 is not renderable.
   It maybe non-power-of-2 and have incompatible texture filtering or
   is not 'texture complete'.
```
解决这个问题只需要将包裹模式设置为`CLAMP_TO_EDGE`并且通过设置过滤器为`LINEAR`或`NEAREST`来解决。
以下是更新后的加载图像的代码

```javascript
// 异步加载图像
var image = new Image();
image.src = "resources/keyboard.jpg";
image.addEventListener('load', function() {
  // 现在有了图像，拷贝到纹理
  gl.bindTexture(gl.TEXTURE_2D, texture);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA,gl.UNSIGNED_BYTE, image);
 
  // 检查每个维度是否是 2 的幂
  if (isPowerOf2(image.width) && isPowerOf2(image.height)) {
     // 是 2 的幂，一般用贴图
     gl.generateMipmap(gl.TEXTURE_2D);
  } else {
     // 不是 2 的幂，关闭贴图并设置包裹模式为到边缘
     gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
     gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
     gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  }
}
```

还有一个问题是：如何为立方体每个面设置不同的纹理？

最好的方法就是将图像放在一个纹理中，然后利用纹理坐标映射不同的图像到每个面， 这是很多高性能应用（读作游戏）使用的技术。例如我们将所有的图像放入这样一个纹理中。比如这样
```javascript
// 选择左下图
    0   , 0  ,
    0   , 0.5,
    0.25, 0  ,
    0   , 0.5,
    0.25, 0.5,
    0.25, 0  ,
    // 选择中下图
    0.25, 0  ,
    0.5 , 0  ,
    0.25, 0.5,
    0.25, 0.5,
    0.5 , 0  ,
    0.5 , 0.5,
    // 选择中右图
    0.5 , 0  ,
    0.5 , 0.5,
    0.75, 0  ,
    0.5 , 0.5,
    0.75, 0.5,
    0.75, 0  ,
    // 选择左上图
    0   , 0.5,
    0.25, 0.5,
    0   , 1  ,
    0   , 1  ,
    0.25, 0.5,
    0.25, 1  ,
    // 选择中上图
    0.25, 0.5,
    0.25, 1  ,
    0.5 , 0.5,
    0.25, 1  ,
    0.5 , 1  ,
    0.5 , 0.5,
    // 选择右上图
    0.5 , 0.5,
    0.75, 0.5,
    0.5 , 1  ,
    0.5 , 1  ,
    0.75, 0.5,
    0.75, 1  ,
```

专业将多个图像通过一个纹理来提供的方法，叫做纹理图集，因为只需要加载一个贴图，着色器也会因为只加载一个贴图而保持简单。

#### webGL内置函数

参考xmind整理


### 回顾MVP矩阵

- Model transformation (placing objects)

  想象一下：世界坐标系下有很多Object，用一个变化矩阵把它们的顶点坐标从Local坐标系（相对）转换到世界Global坐标系（绝对）。这就是placing objects

- View transformation (placing camera)

  想象一下：我们看到的画面由摄像机捕捉，摄像机参数决定了我们在屏幕上看到的东西，这一步可以将世界坐标系转换到摄像机坐标系。

- Projection transformation

  摄像机坐标系，视锥体，再规整一下

- model transformation: 将全部的局部坐标转化为世界坐标（即由物体的局部坐标转化为全局坐标系）-放好物体

- view transformation:摆好相机：将相机和物体一起变换，使得相机位于标准位置处，相机位于原点处，头顶为y轴，向-z轴看去

- projection transformation:投影变换，将世界坐标系转为平面坐标

关于投影变换：具有正交投影和透视投影两种

①正交投影：直接把z轴丢弃，将生成的矩阵平移并缩放为[-1,1]^2。

![img](https://pic4.zhimg.com/80/v2-a8a0135684e9cdf46f8aba815f16cf4b_720w.webp)

②透视投影:具有近大远小的性质

#### 2.1 shadow map的计算

##### `DirectionalLight中的calcLightMVP`

用到的mat4的API有：

LookAt矩阵

(static) **lookAt**(out, eye, center, up) → {mat4}

Generates a look-at matrix with the given eye position, focal point, and up axis. If you want a matrix that actually makes an object look at another object, you should use targetTo instead.

Parameters:

| Name     | Type         | Description                              |
| :------- | :----------- | :--------------------------------------- |
| `out`    | mat4         | mat4 frustum matrix will be written into |
| `eye`    | ReadonlyVec3 | Position of the viewer                   |
| `center` | ReadonlyVec3 | Point the viewer is looking at           |
| `up`     | ReadonlyVec3 | vec3 pointing up                         |

平移矩阵

(static) translate(out, a, v) → {mat4}

Translate a mat4 by the given vector

Parameters:

| Name  | Type         | Description             |
| :---- | :----------- | :---------------------- |
| `out` | mat4         | the receiving matrix    |
| `a`   | ReadonlyMat4 | the matrix to translate |
| `v`   | ReadonlyVec3 | vector to translate by  |

缩放矩阵

(static) scale(out, a, v) → {mat4}

Scales the mat4 by the dimensions in the given vec3 not using vectorization

Parameters:

| Name  | Type         | Description                     |
| :---- | :----------- | :------------------------------ |
| `out` | mat4         | the receiving matrix            |
| `a`   | ReadonlyMat4 | the matrix to scale             |
| `v`   | ReadonlyVec3 | the vec3 to scale the matrix by |

正交投影矩阵

(static) ortho(out, left, right, bottom, top, near, far) → {mat4}

Generates a orthogonal projection matrix with the given bounds

Parameters:

| Name     | Type   | Description                               |
| :------- | :----- | :---------------------------------------- |
| `out`    | mat4   | mat4 frustum matrix will be written into  |
| `left`   | number | Left bound of the frustum（视锥左边界）   |
| `right`  | number | Right bound of the frustum（视锥右边界）  |
| `bottom` | number | Bottom bound of the frustum（视锥下边界） |
| `top`    | number | Top bound of the frustum（上）            |
| `near`   | number | Near bound of the frustum（近）           |
| `far`    | number | Far bound of the frustum（远）            |

----

关于LookAt()

lookat矩阵是由这些东西来定义的：摄像头位置，它看向的方向，以及向上的up方向。由于“OpenGL中我们知道摄像机指向z轴负方向”，所以给图如下：

![img](https://pic1.zhimg.com/80/v2-ad4b6f56e2133c1dd2f62e0bc32325fc_720w.webp)

在Three.js中不需要关心这个矩阵是怎么获得的，只需要知道一个视点的具体位置即(eyeX,eyeY,eyeZ),观测点的位置即Shading Point位置(atX,atY,atX),相机的上方向，即

(upX，upY, upZ)

---

 需要完善`Direction函数中的CalcLightMVP`

有疑问：传入的translate和scale是哪来的？？

经过编辑器的搜索，这里的translate和scale是位于Mesh.js中的TRSTransform向量

```javascript
class TRSTransform {
    constructor(translate = [0, 0, 0], scale = [1, 1, 1]) {
        this.translate = translate;
        this.scale = scale;
    }
}
```

第一次先将正交投影矩阵设置成这个样子看看效果

```javascript
 CalcLightMVP(translate, scale) {
        let lightMVP = mat4.create();
        let modelMatrix = mat4.create();
        let viewMatrix = mat4.create();
        let projectionMatrix = mat4.create();

        // Model transform
        // 先根据translate向量创建一个平移矩阵
        mat4.translate(modelMatrix, modelMatrix, translate);
        // 在根据scale向量在modelMatrix基础上进行缩放
        mat4.scale(modelMatrix, modelMatrix, scale);
        
        // View transform
        // 设置相机的位置
        mat4.lookAt(viewMatrix, this.lightPos, this.focalPoint, this.lightUp);
    
        // Projection transform
        // 设置投影矩阵（根据作业设置的是一个正交投影矩阵）
        // 根据作业是相机离物体越远那么，那么对应的shadowmap的范围也就越大
        mat4.ortho(projectionMatrix, -200, 200, -200, 200, -100, 100)


        mat4.multiply(lightMVP, projectionMatrix, viewMatrix);
        mat4.multiply(lightMVP, lightMVP, modelMatrix);

        return lightMVP;
    }
```

----

实现phong片元着色器中的`useshadowmap`

①了解在GLSL中的attribute和uniform和varying的区别

在GLSL（OpenGL Shading Language）中，`attribute`、`uniform` 和 `varying` 关键字用于声明不同类型的变量，它们在着色器程序中发挥着不同的作用：

1. **`attribute`**:
   - **用途**: `attribute` 变量通常用于顶点着色器（Vertex Shader），用来表示每个顶点的数据，如顶点坐标、法线、纹理坐标等。
   - **特点**: 每个顶点都有自己的一套 `attribute` 数据，这些数据通常来自于应用程序传递给GPU的缓冲区。
   - **生命周期: `attribute` 变量仅在顶点着色器中可用。**

2. **`uniform`**:
   - **用途**: `uniform` 变量在顶点和片元着色器中都可以使用，用来表示整个渲染过程中保持不变的全局数据，如变换矩阵、光源信息、材质属性等。
   - **特点**: `uniform` 变量对于一次渲染调用中的所有顶点和所有片元是一致的（即不变的）。
   - **生命周期: `uniform` 变量在整个着色器程序中都是有效的，直到下一次渲染调用时可能被更新。**

3. **`varying`** (在较新的GLSL版本中被 `in` 和 `out` 关键字所取代):
   - **用途**: `varying` 变量用于在顶点着色器和片元着色器之间传递数据。它们通常用于输出顶点着色器计算的数据，并在片元着色器中接收这些数据。
   - **特点**: 顶点着色器为每个顶点设置 `varying` 变量的值，然后这些值会被插值（例如线性插值）后传递给片元着色器。例如，用于计算光滑着色或纹理坐标插值。
   - **生命周期: `varying` 变量在顶点着色器中被赋值，在片元着色器中被使用。**

----

关于OpenGL中渲染管线的输入与输出？

①对于顶点着色器的输出：输出就是在裁剪空间上的坐标(Clip Space)。

②由GPU自己实现由裁剪空间到NDC（标准化设备坐标）的映射，也即是直接除以裁剪空间坐标的w分量（是一个负值分量）即可

③为什么叫裁剪坐标，因为②除以一个负的分量的结果，会把三个xyz坐标映射到[-1,1]范围内，不在范围内的坐标就会被裁剪

④由于在原本的裁剪空间是标准的右手系，由于除以一个负的分量后，变为了标准的左手系

⑤对于片元着色器的输入是什么？不是裁剪空间的坐标，也不是经过了NDC变换后的标准设备坐标。而是将NDC坐标在经过视口变换（view port)转换为了屏幕坐标（window space)(screen space)

⑥对于片元着色器的输入的屏幕坐标，他的范围就在定义的可视平面的范围内。即

以x轴为例，变换只是将NDC的[-1,1] 线性映射到[x, x+width]范围内。

z轴则会从NDC的[-1,1]映射到[nearVal, farVal]内（默认near=0，far=1）

**现在对于NDC坐标的描述也就清楚多了**

----

##### shadowmap的基本原理

相比较于对场景进行无阴影的渲染，ShadowMap阴影流程需要多一个Pass。

第一个Pass就是从光源（从光源的正中心看作点光源）将点光源看作放置了一个Camera，从点光源方向作为视角渲染一张深度图。得到ShadowMap。具体怎么做呢？就是将场景经过MVP变换（MV:modelviewtransform得到的是Eye Coordinates)(P:projection transform得到的是clip coordinates)。将clip coordinates经过透视除法得到NDC坐标。按照作业要求转为NDC标准空间后，即可得到再光源视角下得到场景中所有点的坐标。由坐标的z值即可得到从光源看向场景的深度值

第二个Pass再从真实的摄像机渲染真实的场景，再渲染场景时，把渲染场景时把像素点变换到相机空间中，取其再相机空间中的深度与在shadowmap中同一个uv坐标所记录的深度做对比，若大于shadowmap上记录的深度则该点位于阴影中

##### unpack函数（需要配合pack函数使用）

一：在`shadowFragment.glsl`中存在着`pack`函数

```glsl
#ifdef GL_ES
precision mediump float;
#endif

uniform vec3 uLightPos;
uniform vec3 uCameraPos;

varying highp vec3 vNormal;
varying highp vec2 vTextureCoord;

vec4 pack (float depth) {
    // 使用rgba 4字节共32位来存储z值,1个字节精度为1/256
    const vec4 bitShift = vec4(1.0, 256.0, 256.0 * 256.0, 256.0 * 256.0 * 256.0);
    const vec4 bitMask = vec4(1.0/256.0, 1.0/256.0, 1.0/256.0, 0.0);
    // gl_FragCoord:片元的坐标,fract():返回数值的小数部分
    vec4 rgbaDepth = fract(depth * bitShift); //计算每个点的z值
    rgbaDepth -= rgbaDepth.gbaa * bitMask; // Cut off the value which do not fit in 8 bits
    return rgbaDepth;
}

void main(){

  //gl_FragColor = vec4( 1.0, 0.0, 0.0, gl_FragCoord.z);
  gl_FragColor = pack(gl_FragCoord.z);
}
```

对应的解码代码unpack为（unpack位于`PhongFragment.glsl`中）

```glsl
float unpack(vec4 rgbaDepth) {
    const vec4 bitShift = vec4(1.0, 1.0/256.0, 1.0/(256.0*256.0), 1.0/(256.0*256.0*256.0));
    return dot(rgbaDepth, bitShift);
}
```

为什么要进行编码和解码？

1. **存储和传输效率**：通过将多个数据值合并成一个更紧凑的格式（如将浮点数打包到RGBA颜色值中），可以更有效地利用存储空间和传输带宽。这在处理大量数据，如纹理、深度信息或其他图形数据时特别重要。
2. **利用现有的硬件和算法**：现代图形处理单元（GPUs）和图像处理库通常针对处理标准图像格式（如RGBA）进行了优化。将非图像数据（如深度信息、法线、物理量等）打包到这些格式中，可以充分利用这些优化，从而提高处理速度和效率。
3. **精度和范围调整**：某些类型的数据可能具有超出标准图像格式所能直接表示的范围或精度。通过打包和解包，可以将这些数据映射到可表示的范围内，同时尽可能保留原始数据的精度。
4. **渲染管线的兼容性**：在图形渲染管线中，某些阶段（如像素着色器）可能只能输出特定格式的数据。通过打包，可以将多种数据合并为一种格式，以适应这些限制。
5. **避免数据的多次传递**：将多个数据打包到一起可以减少对不同数据的多次传递需求，提高数据处理的效率和速度。

----

看了几天，终于把作业框架大致看懂了

中间看了

webGL基础

GLSL语法

Javascript基础（promise，AJAX）

GL-MATRIX运算

THREE.js的模型加载部分

----

### 总结

   ![作业一整理](D:\BaiduNetdiskWorkspace\games202\作业一整理.png)

![作业一整理二](D:\BaiduNetdiskWorkspace\games202\作业一整理二.png)

### `useShowMap`静态Bias

![useshadowmap_1](D:\BaiduNetdiskWorkspace\games202\useshadowmap_1.png)

自遮挡现象：由于数据精度造成的（即）

![7](D:\BaiduNetdiskWorkspace\games202\3\7.png)

第一次生成shadowmap，即从光源视角看向场景，并再shadowmap的每个像素上记录一个深度信息，可以看到shadowmap的一个像素会对应着场景中的一片区域。再第二趟渲染中，从视角看去。从shading point往光源连线，穿过的像素和黑线记录深度的像素是一个像素，找到对应的深度信息，虽然再场景中是不同的区域，但是对应再shadowmap上是同一个记录深度的像素，因此判断shadingpoint被遮挡了。

**关于使用shadowMap中的一个细节：**

- 第一个Pass是先从光源点向场景中看去，渲染出一张深度图，得到所谓的ShadowMap，没有疑问

- 第二个Pass是从摄像机渲染场景，但，渲染时，需要将像素点Shading Point转换到光源空间下（为什么呢？因为在第一趟记录的是光源场景下的深度，那么第二趟渲染时比较的深度也应该是在光源坐标下的深度）

- 第二点体现在哪呢？也就是`PhongVertexShader`中

  ` vPositionFromLight = uLightMVP * vec4(aVertexPosition, 1.0);`通过光源的MVP变化，即这里的`aVertexPosition`是相机视角下的空间坐标，通过变换，变化到了光源视角的空间坐标

- 那么在使用shadowMap的时候，`vPositionFromLight`的z坐标值就是在光源坐标空间下的$d_{receiver}$

解决自遮挡现象：添加一个bias偏差，在记录的深度和真实深度之间的插值大于一个bias时才认为这个像素处于阴影之中。

上述结果就是添加静态偏差之后的结果，发现也有问题，问题在于：这里的bias是完全固定的。但是实际场景中这个阈值是可以动态变化的，比如，在光源倾斜照向地板时，这时候可以把阈值设的稍微大一点，在光源垂直照向地板时，这时候可以把阈值设的稍微小一些。

### 动态Bias(自适应Bias算法)

- 阴影瑕疵(shadow acne)的成因及其数学模型
- shadow bias数值的精确计算方法
- 级联阴影(cascade shadow)对shadow bias的影响
- PCF阴影对shadow bias的影响
- Shadow Caster Vertex Based Bias.
- 计算性能优化

#### shadow acne阴影失真原因以及模型

shadow bias是为了解决自遮挡阴影瑕疵(shadow acne)而提出的。 由于shadowmap技术自身的原因，当bias没有加入时，场景中会出现条状阴影纹路，这便是自遮挡阴影瑕疵。

![img](https://pic1.zhimg.com/80/v2-f13b0555a326a87e369a0c27b0ec56c8_720w.webp)

引起阴影失真的原因：

![img](https://pic1.zhimg.com/80/v2-80d24f8bf7ff74dbf2aa0e5d858a65a0_720w.webp)

> 箭头为入射的平行光，每一条斜黄线段代表ShadowMap贴图中的一个像素，对应到水平地面上可能覆盖多个像素。多个像素里有些深度较大，有些深度较小，因而产生了条状瑕疵。

#### 数学模型

![img](https://pic2.zhimg.com/80/v2-9716e7f65ca610c04605f6e252a3ce01_720w.webp)

- 橙色线条为平行光视角的近平面，记为L，这个近平面最终将映射为一张ShadowMap深度图。
- 蓝色线条为接受光照的平面。

我们暂且假设Light对应的正交矩阵其宽高相同(实际上URP中就是这样的),记为frustumSize。ShadowMap贴图的尺寸记为shadowMapSize。

设图中AB线段代表ShadowMap贴图上单位像素在Light近平面上对应的尺寸。那么我们有以下公式:

```text
|AB| = frustumSize / shadowMapSize
```

> 这里
>
> frustumsize：指的是视锥体的大小（一般都是正交投影，视锥体的宽高相同）
>
> shadowMapSize：阴影贴图的尺寸。这是阴影贴图的宽度或高度，通常以像素为单位。
>
> **根据相似三角形的原理**，阴**影贴图上的一个像素对应光源视锥体上的一个区域，其大小与光源视锥体的大小成比例**。因此，我们可以通过将光源视锥体的大小（frustumSize）除以阴影贴图的尺寸（shadowMapSize）来计算单位像素在光源近平面上的投影尺寸。

从AB作橙线的垂线，相交场景蓝色平面于CD两点。易知，CD范围内的所有像素均投影到平面L上的AB区域。

由于AB代表了ShadowMap上的单个像素，因此AB像素中存储的深度值应当是CD中点F到平面L的距离，即|FE|(实际上是归一的)。

我们记:

`Distance(X,L)` - 表示任意点X到光源近平面L的距离。

那么显然:

- 对任意点X属于CF,有Distance(X,L) < |FE|，因此判定为不在阴影中
- 对任意点X属于DF，有Distance(X,L) > |FE|，因此判定为在阴影。

于是在CD区域中会呈现出一半白，一半黑的阴影瑕疵，这就是Shadow Acne，其以CD长度为周期在平面上循环交替，而|CD|正是ShadowMap中单个像素投射在场景上覆盖的区域。

#### shadowBias

既然已经知道了问题所在，我们就可以在深度对比阶段，来修正这个问题。

**Depth Bias**

![img](https://pic1.zhimg.com/80/v2-57ebd76073242934b3bd6674162f5698_720w.webp)

**我们只要将CD中的点，往光照方向平移一定距离到GH上即可修正误差**。(统一将CD上点往光源反方向移动GD，这样原本CF再光照中依旧是光照，原本再阴影的FD因为移动而处于光照之中）这个移动的距离即称为Depth Bias。**不妨先考察D点。 |DG|即是要修正的Depth Bias**，于是我们有:

$DepthBias(D)=|DG|=|FG|tan\theta=\frac{|AB|}{2}tan\theta$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           

$DepthBias(D) = \frac{frustumSize*tan\theta}{shadowMapSize*2}$

那么对于CD上的任意点X，我们可以使用相似三角形的原理，从DepthBias(D)乘以对应的比例就可以了。(也就是说这里DG是最长的DepthBias，之后再CD之间的DepthBias(x)只要再最长的基础上乘以一个比例即可)

伪代码

```
shadowUV = shadowProj(X);
percent = (frac(shadowUV) - 0.5) / 0.5;
DepthBias = DepthBias_D * percent;
```

实际上在大多数引擎的实现中，并不会这么精确的去计算每个点的bias数值，而是对所有的点执行一个固定的Depth Bias。很明显，**D点的误差是最大的，因此只要对所有的点都使用D点的bias数值，就可以修正自遮挡的问题。**

**但是固定的depth bias也会引起其他的问题。**

#### 漏光问题

![img](https://pic3.zhimg.com/80/v2-6b1339b5bee20f2f13c42c5ca6ebb9f6_720w.webp)

考虑点C前面有个遮挡物。

本来C点应该是处于阴影中的(因为有遮挡物存在）。但是我们通过depth bias将C点往光源方向进行了长度为|DG|的偏移（即将C往近平面移动），那么C点就变到了遮挡物前面去了。

#### bias趋于无穷大的问题

当入射光线与平面夹角趋于0，即 $\theta$ 趋于90度时，线段DG长度会趋于无穷大。从|DG|的公式也可以看出:

$DepthBias(D) = \frac{frustumSize*tan\theta}{shadowMapSize*2}$

![img](https://pic2.zhimg.com/80/v2-cf67e935092d58b7b3d4cc8cc1391315_720w.webp)

因此固定尺寸的depth bias在$\theta $区域90度时。一定会失效

#### Normal Bias

为了解决Depth Bias的缺陷，于是人们提出了Normal Bias。顾名思义，既然往光源方向偏移有问题，那么我就往法线方向偏移呗。![img](https://pic4.zhimg.com/80/v2-3c945052f9ba0cb14a94df69e7204d7b_720w.webp)

将CD平面按照平面的法线方向进行平移，平移的距离为|GM|。

由图可以看出

- C'G段上的点的最大深度为BG（==EF）因此C'G段的点，深度都小于EF，因此光亮
- D'G段上的点，移动到了隔壁像素区间上，也可以得知是小于隔壁像素深度的，也是光亮的
- $NormalBias(D) = \frac{frustumSize*sin\theta}{shadowMapSize*2}$
- 于Depth Bias相比，其优点在于当 $\theta$趋于90度时，bias趋于`{frustumSize/(2 * shadowMapSize)}`，而不是无穷大。也即，normal bias的最大值只与shadowMap的分辨率有关。

#### NormalBias的漏光

Normal Bias同样会存在漏光问题，并且是两头漏光。考虑有个遮挡物如下:![img](https://pic4.zhimg.com/80/v2-1f04b4af9e74b83315d61385fa2fed1b_720w.webp)该遮挡本应该在CD区域投下阴影。但由于normal bias的存在，我们在进行深度判定的时候将CD移到了C'D'，因此CD平面左侧会有少部分位于遮挡物上方，从而漏光。而右侧的GD'部分，也因为进入了隔壁的像素地盘，形成了漏光。

但在实际应用中，由于Normal Bias的最大值相对可控(只要提升ShadowMap分辨率即可)，因此漏光问题并不太严重。

#### 级联阴影对Bias的影响

根据前面推导的公式，我们已知影响最大Bias的两个因素如下:

- ShadowMap的分辨率（也就是ShadowMapSize)
- 平行光的入射角(也就是影响角度$\theta$)

级联阴影的策略就是将摄像机视锥按照不同的距离区间划分为多块，每块采用不同的分辨率的shadowmap，称为一级。因此，在使用级联阴影时，入药计算场景中某个点的bias,必须要考虑到他是属于哪一个级别，计算出该级别的ShadowMap的分辨率之后，确定最后的bias

![img](https://pic1.zhimg.com/80/v2-925a88ada00edaed58ec7ef12509d554_720w.webp)

>  开启CSM情况下的Shadow Acne示意图

在开启4级CSM的情况下，由近及远，由于每级ShadowMap分辨率不一样，因此自遮挡引起的条纹也呈现出不同的分布密度。

假如我们对整个场景都使用相同的bias而没有针对CSM每个级别单独计算，那么会出现以下情况:

![img](https://pic4.zhimg.com/80/v2-fa3ea0fa16d85e5d0fe129fc111097fb_720w.webp)

近处的瑕疵消失了，但远处依旧会出现。如若我们为了照顾最远级别的CSM而将Bias调的很大，那么近处的漏光现象就会变得严重。

**因此自适应的Bias算法需要考虑场景像素所在的CSM级别，并按照相应的CSM分辨率进行计算。**

#### PCF对Bias的影响

`PCF(Percentage Closer Filtering)`:之前在做任何一个点在不在阴影中的判断（原本的做法是将这点连向light,在与shadowmap上这一点深度进行比较，只做了唯一一次比较），PCF的区别：投影到light之后不是找对应的唯一一个像素点，而是找周围一圈的像素点，把周围的每一个深度都与这个点的深度进行比较（卷积思想）。做完之后将所有得到的非零即一的结果平均起来作为可视度Visibility

---

PCF会采样一个范围内的像素，因此我们必须考虑这个范围内距离光源最近的那个像素。

例如对于PCF1,它会采样ShadowMap上的4个像素，依次进行深度对比测试。【PCF Shadow Map 几何模型示意图】![img](https://pic1.zhimg.com/80/v2-2005bcf6ccd81d7810ebb72a24123aac_720w.webp)

当我们计算CF区间内的像素阴影时，不仅会采样像素AB对应的Shadow Map, 同样会采样到 AN 对应的 Shadow Map像素。（AB与AN分别为一个像素）

此时，取GD作为Depth Bias无法再满足要求。我们必须让偏移后的点到$L_{near}$的距离要小于OP距离（此时才满足再两个像素都是光亮），由于EF时OP的两倍，根据相似三角形，只要将Depth Bias取为$2 * Depth Bias$，才能使得CF中的点到L近平面的距离小于OP。

易知，该推导可以扩展到任意采样半径R。即：

$NormalBias(D) = （1+ceil(R))\frac{frustumSize*sin\theta}{shadowMapSize*2}$

$DepthBias(D) = （1+ceil(R))\frac{frustumSize*sin\theta}{shadowMapSize*2}$

- ceil为向上取整函数

#### 常见的计算优化

在实际的实现中，为了避免三角函数运算，通常可以使用 `1 - dot(lightDir,normal)`来近似代替tan和sin，然后暴露出两个系数$C_{depth}$和$C_{normal}$ ，给予使用者进行调整。

于是公式如下

- $A = (1+ceil(R))*\frac{frustumSIze}{shadowMapSize *2}$
- $B=1-dot(lightDir,normal)$
- $DepthBias=C_{depth}*A*B$
- $NormalBias=C_{normal}*A*B$​
- $lightDir为光照方向$
- $由于\theta根据上图可以得知\theta=arcos(dot(lightdir,normal))$
- $normal是片元的normal$

----

在作业框架中实现自适应的bias计算

- shadowmap的size即在`engine`中所定义的`resolution`变量信息

- frustumsize为视锥的宽高，直接定义

  ![useshadowmap_2_bias](D:\BaiduNetdiskWorkspace\games202\useshadowmap_2_bias.png)

  此时c设置为0.4， filterRadis设为1

  ----

  ![useshadowmap_3_bias](D:\BaiduNetdiskWorkspace\games202\useshadowmap_3_bias.png)

c设为0.4，filterradius设为1.7

filterradius设为0.7效果也差不多

---

### PCF

#### 泊松圆盘采样

目标：需要在一个宽和高为（width,height)的平面内平均生成一堆点，要求这些点之间的距离不能小于采样半径R。即如果以每个点为圆心，半径为R，那么每个圆内都不应该出现其他的采样点。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210703150628960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxNDc2OTUz,size_16,color_FFFFFF,t_70)

方法：

- 暴力方法：生成一堆点，两两判断点之间距离是否满足条件，这样时间复杂度就是$$O(N^2)$$ 
- 创建一个网格，网格的对角线为必须保持的距离R
- 为什么要这么做呢？意味着无论点在单元格中的那个位置，以R为半径的圆始终可以覆盖点所在的单元格![泊松圆盘采样一](D:\BaiduNetdiskWorkspace\games202\泊松圆盘采样一.png)
- 由此意味着，一个单元格内只可以出现一个点
- ![泊松圆盘采样二](D:\BaiduNetdiskWorkspace\games202\泊松圆盘采样二.png)由此也可看出，单元格内点为重心的圆不可能超过以单元格为中心的5*5的网格，那么，每次判断两个点距离（新点和已经生成的点)只需要判断网格内的点即可，没在网格内的点距离一定满足要求

算法步骤：

1. 从激活点集中随机选出一个点x（激活点集是已采样点集的子集）
2. 在以该点为中心，内半径为r/外半径为2r的圆环上随机采k个点（推荐k=30）
3. 对k个点进行逐一检查，如果某个点与所有已经采样的点距离都大于r，则将该点放入激活集）（并且一旦出现检查点与采样点集中某个点距离小于r,break就行之后就没意义了）(在以x的采样点集中的k个点都没有满足要求的，将x移除激活点集)
4. 采样点集合存储最终采样的点。激活点集合类比于判定队列

**对于第三步中判定k个点中某一个点与已经采样的所有点的距离，这一步可以用网格进行简化，只需要判定这个点周围5*5的网格即可。因为不在这个范围内的点一定满足条件（即距离一定大于r)。**

python实现：

```python
import numpy as np
import matplotlib.pyplot as plt

r = 1.0
d = r / np.sqrt(2)
k = 30
width = 20
height = 16
nx = int(width / d) + 1
ny = int(height / d) + 1

occupied = np.zeros((nx, ny))
occupied_coord = np.zeros((nx, ny, 2))
active_list = []
sample_list = []

relative = np.array([[-2, 2], [-1, 2], [0, 2], [1, 2], [2, 2],
                     [-2, 1], [-1, 1], [0, 1], [1, 1], [2, 1],
                     [-2, 0], [-1, 0], [1, 0], [2, 0],
                     [-2, -1], [-1, -1], [0, -1], [1, -1], [2, -1],
                     [-2, -2], [-1, -2], [0, -2], [1, -2], [2, -2]])

np.random.seed(0)
x, y = np.random.rand() * width, np.random.rand() * height
idx_x, idx_y = int(x / d), int(y / d)
occupied[idx_x, idx_y] = 1
occupied_coord[idx_x, idx_y] = (x, y)
active_list.append((x, y))
sample_list.append((x, y))

# 计数
sampled_idx = 0

while len(active_list) > 0:
    
    # 取激活集一个点，随机生成它周围的k个点
    idx = np.random.choice(len(active_list))
    ref_x, ref_y = active_list[idx]
    radius = (np.random.rand(k) + 1) * r
    theta = np.random.rand(k) * 2 * np.pi
    candidate = ref_x + radius * np.cos(theta), ref_y + radius * np.sin(theta)
    
    flag_out = False
    
    # k个点中的一个点确定了，开始检查它周围的点， 看这个点是否可以放进去集合中
    for _x, _y in zip(*candidate):
        if _x < 0 or _x > width or _y < 0 or _y > height:
            continue
        flag = True
        # 该点网格坐标
        idx_x, idx_y = int(_x / d), int(_y / d)
        if(occupied[idx_x, idx_y] == 1):
            # 该网格已经有点了
            continue
        # 5 * 5 邻域
        else:
            neighbors = relative + np.array([idx_x, idx_y])
        
        # 检查这个点是否可以放进去
        for cand_x, cand_y in neighbors:
            if cand_x < 0 or cand_x >= nx or cand_y < 0 or cand_y >= ny:
                continue
            if occupied[cand_x, cand_y] == 1:
                cood = occupied_coord[cand_x, cand_y]
                if (_x - cood[0]) ** 2 + (_y - cood[1]) ** 2 < r ** 2:
                    flag = False
                    break
        
        if flag:
            # 中心点周围有满足条件的点，不用弹出
            flag_out = True
            occupied[idx_x, idx_y] = 1
            occupied_coord[idx_x, idx_y] = (_x, _y)
            active_list.append((_x, _y))
            sample_list.append((_x, _y))
            sampled_idx += 1
            break
    if not flag_out:
        active_list.pop(idx)
                
fig, ax = plt.subplots(1, 1, figsize=(9, 6))
fig.set_tight_layout(True)
ax.scatter(*zip(*sampled), c='g')
ax.set_xlim([0, width])
ax.set_ylim([0, height])
plt.show()          
```

![泊松圆盘采样三](D:\BaiduNetdiskWorkspace\games202\泊松圆盘采样三.png)

用树结构来看这些点是如何生成的

![泊松圆盘采样四](D:\BaiduNetdiskWorkspace\games202\泊松圆盘采样四.gif)

注：

- 星号（*）运算符的作用是将一个可迭代对象解包成独立的元素,这里将元组中包裹的np.array对象解包成立单独的list

- zip打包

  ````python
  `zip(*candidate)` 的操作是将一个包含多个可迭代对象（比如列表、元组等）的元组解压缩，并将这些可迭代对象的元素依次配对。
  
  具体来说，假设 `candidate` 是一个包含多个可迭代对象的元组，如 `(iterable1, iterable2, ..., iterable_n)`，其中每个 `iterable` 可以是列表、元组或其他可迭代对象。那么 `zip(*candidate)` 将会把这个元组解压缩成 n 个独立的可迭代对象，然后将这些可迭代对象的第一个元素、第二个元素、...、第 n 个元素配对起来，形成一个新的迭代器。如果某个可迭代对象的长度比其他对象短，则 `zip` 函数会以最短的可迭代对象的长度为准，忽略多余的元素。
  
  举个例子，假设 `candidate` 是一个包含两个列表的元组：
  
  ```python
  candidate = ([1, 2, 3], ['a', 'b', 'c'])
  ```
  
  那么 `zip(*candidate)` 将会将这两个列表解压缩，并将它们的元素依次配对：
  
  ```python
  [(1, 'a'), (2, 'b'), (3, 'c')]
  ```
  
  这样得到的新的迭代器中每个元素都是一个元组，包含了两个列表中对应位置的元素。
  ````

  

- ```python
  candidate = (np.array([ 9.43395482,  9.57201335, 12.39001486,  9.7598591 ,  9.88075214,
          9.57253696, 12.81855326, 10.40158444,  9.8384556 ,  9.56548208,
         10.4695762 , 12.76563853, 10.44133523, 10.45636386, 11.22761209,
         12.23976857, 10.2656893 ,  9.75101196,  9.18700561,  9.30933898,
         12.4338518 , 12.40318369, 11.26201096, 11.84356142, 10.32241616,
         10.93605379,  9.48838905, 11.02580854, 11.66075121, 12.34067112]), 
               np.array([11.87905061, 10.79903679, 11.61071609, 10.33428653, 10.51217143,
         10.17482703, 10.76335044, 10.18459955, 12.82710241, 12.03226403,
          9.95910767, 12.15442494, 10.51514897, 10.48827982, 12.43180323,
         12.7704586 , 13.07303489, 12.85571808, 10.59836235, 12.12001749,
         11.3363647 , 12.50801892, 12.52418195, 12.83484403, 10.50508967,
         13.38728289, 11.76276784, 12.85682417, 12.50632026, 12.57718813])
               )
  print(next(zip(*candidate)))
  
  # (9.43395482, 11.87905061)
  ```

最终代码,用解包怪怪的，不用解包了

```python
import numpy as np
import matplotlib.pyplot as plt

r = 1.0
d = r / np.sqrt(2)
k = 30
width = 20
height = 16
nx = int(width / d) + 1
ny = int(height / d) + 1

occupied = np.zeros((nx, ny))
occupied_coord = np.zeros((nx, ny, 2))
active_list = []
sample_list = []

relative = np.array([[-2, 2], [-1, 2], [0, 2], [1, 2], [2, 2],
                     [-2, 1], [-1, 1], [0, 1], [1, 1], [2, 1],
                     [-2, 0], [-1, 0], [1, 0], [2, 0],
                     [-2, -1], [-1, -1], [0, -1], [1, -1], [2, -1],
                     [-2, -2], [-1, -2], [0, -2], [1, -2], [2, -2]])

np.random.seed(0)
x, y = np.random.rand() * width, np.random.rand() * height
idx_x, idx_y = int(x / d), int(y / d)
occupied[idx_x, idx_y] = 1
occupied_coord[idx_x, idx_y] = (x, y)
active_list.append((x, y))
sample_list.append((x, y))

# 计数
sampled_idx = 0

while len(active_list) > 0:
    
    # 取激活集一个点，随机生成它周围的k个点
    idx = np.random.choice(len(active_list))
    ref_x, ref_y = active_list[idx]
    radius = (np.random.rand(k) + 1) * r
    theta = np.random.rand(k) * 2 * np.pi
    candidate_x, candidate_y = ref_x + radius * np.cos(theta), ref_y + radius * np.sin(theta)
    print(candidate)
    
    flag_out = False
    
    # k个点中的一个点确定了为_x,_y，开始检查它周围的点， 看这个点是否可以放进去集合中
    for _x, _y in zip(candidate_x, candidate_y):
        #print(_x, _y)
        if _x < 0 or _x > width or _y < 0 or _y > height:
            continue
        flag = True
        # 该点网格坐标
        idx_x, idx_y = int(_x / d), int(_y / d)
        if(occupied[idx_x, idx_y] == 1):
            # 该网格已经有点了
            continue
        # 5 * 5 邻域
        else:
            neighbors = relative + np.array([idx_x, idx_y])
        
        # 检查这个点是否可以放进去
        for cand_x, cand_y in neighbors:
            if cand_x < 0 or cand_x >= nx or cand_y < 0 or cand_y >= ny:
                continue
            if occupied[cand_x, cand_y] == 1:
                cood = occupied_coord[cand_x, cand_y]
                if (_x - cood[0]) ** 2 + (_y - cood[1]) ** 2 < r ** 2:
                    flag = False
                    break
        
        if flag:
            # 中心点周围有满足条件的点，不用弹出
            flag_out = True
            occupied[idx_x, idx_y] = 1
            occupied_coord[idx_x, idx_y] = (_x, _y)
            active_list.append((_x, _y))
            sample_list.append((_x, _y))
            sampled_idx += 1
            break
    if not flag_out:
        active_list.pop(idx)
                
fig, ax = plt.subplots(1, 1, figsize=(9, 6))
fig.set_tight_layout(True)
ax.scatter(*zip(*sampled), c='g')
ax.set_xlim([0, width])
ax.set_ylim([0, height])
plt.show()          
```

#### 均匀圆盘采样

**实现一：正方形采样**

可以在x从[-1,1],y从[-1,1]的正方形内均匀采样，然后把不在圆内的点剔除即可。

非常简单的采样方法，称为“拒接采样”或者“接受-拒绝采样”。我们再试着计算采样效率，也就是落在单位圆内概率$\frac{\pi}{4}$

看起来效率比较低，这是拒绝采样的弊端

**实现二：逆变换法计算概率分布函数**

逆变换定理：

> X是一个随机变量，其PDF为$f_X(x),CDF为F_X(x)$,并且已知$Y=g(X)，且g有唯一的逆g^{-1}(~)$ 。
>
> 则$f_Y(y)=f_X(x)|\frac{dx}{dy}|$
>
> 在X，Y是向量情况下
>
> 则$f_Y(y)=f_X(x)|detJ_{xy}|$

Box_Muller Method生成随机数定理：

例如要生成一个服从指数分布的随机变量，我们知道指数分布的概率分布函数 (CDF) 为$F(x)=1–e^{–λx}$，其反函数为$F^{−1}(x)=−\frac{ln(1−x)}{λ}$​​，所以只要不断生成服从(0,1)均匀分布的随机变量，代入到反函数中即可生成指数分布。

> 即证明：当
>
> $X-U(0,1),则Y=-\frac{ln(1-x)}{λ}服从参数为λ的指数分布$
>
> $F_Y(y)=P(Y<=y)=P(-\frac{ln(1-x)}{λ}<=y)=反函数$
>
> $=P(X<=1-e^{λy})$
>
> 利用均匀分布展开可得。得证

逆变换定理用语言来叙述就是，找到**要变换的变换后的随机变量的概率分布函数**，**其反函数就是我们要找的变换**，这个变换能把均匀分布映射到已知的概率分布上。

若$U_1,U_2$为标准的[0,1]上的均匀分布，那么通过下式计算的$X,Y$为标准的正态分布且相互独立

$X=\sqrt{-2lnU_1}cos(2\pi U_2)$

$Y=\sqrt{-2lnU_1}sin(2\pi U_2)$

利用逆变换法生成正态分布：怎么生成这个变换的呢？就是要找变换后随机变量的概率分布函数，他的反函数就是要找的变换。

第一步就是求变换后的概率分布函数：

变换后X，Y相互独立，且都是均值为0，方差为1的随机变量，因此

$P(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$

$P(y) = \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}$

联合分布为

$P(x,y) =p(x)p(y)= \frac{1}{2\pi}e^{-\frac{x^2+y^2}{2}}$

根据极坐标变换

$x = Rcos\theta$,$y = Rsin\theta$

$P(R,\theta)=\frac{1}{2\pi}e^{-\frac{r^2}{2}}$

由此可以计算边缘概率分布函数

$F_{\phi}(\phi)=P(\theta<=\phi)=\int_{0}^{\phi}\int_{0}^{+\infty}\frac{1}{2\pi}e^{-\frac{r^2}{2}}rdrd\theta=\frac{\phi}{2\pi}$

$F_{r}(R)=P(R<=r)=\int_{0}^{r}\int_{0}^{2\pi}\frac{1}{2\pi}e^{-\frac{r^2}{2}}rdrd\theta=1-e^{\frac{r^2}{2}}$

由此求出了变换后的概率分布函数

那么求其反函数就是其变换

$\phi = 2\pi F_{\phi}^{-1}(\phi)=带入U_1=2\pi U_1$​

即

$\theta = 2\pi F_{\theta}^{-1}(\theta)=带入U_1=2\pi U_1$

$R=F^{-1}(U_2)=\sqrt{-2lnU_2}$

换回直角坐标有

$X=Rcos\theta=\sqrt{-2lnU_2}cos(2\pi U_1)$

$Y=Rsin\theta=\sqrt{-2lnU_2}sin(2\pi U_1)$

----

与均匀圆盘采样有什么关系呢？

```python
import numpy as np
import matplotlib.pyplot as plt
import random
import math

plt.figure(figsize=(12, 12), dpi = 80)
plt.subplot(1, 1, 1)

# 画圆
theta = np.linspace(0, 2 * np.pi, 800)
x,y = np.cos(theta), np.sin(theta)
plt.plot(x, y, color = 'red', linewidth = 2.0)

# 画随机点
a = []
b = []

for i in range(10000):
    theta = random.uniform(0, 2 * np.pi)
    r = random.uniform(0, 1)
    a.append(r * math.cos(theta))
    b.append(r * math.sin(theta))


plt.xlim(xmax = 1, xmin = -1)
plt.ylim(ymax = 1, ymin = -1)

plt.scatter(a, b, s = 1.5, color = (0., 0.5, 0.))

plt.show()
```

![均匀圆盘采样1](D:\BaiduNetdiskWorkspace\games202\均匀圆盘采样1.png)

从上图可以看出，中心点更加密集，说明靠近中心点的采样概率越大。因为这里是均匀采样半径的，对于采样到的$r$,这一个圆的周长是$2\pi r$，这意味着不同周长的采样概率相等，但是理想的是周长越大则应该被采样更多的点。即采样概率与半径有关，但不能是简单的线性关系。

因此，需要计算单位圆的均匀采样公式。

同上，我们有两个服从均匀分布的随机变量$U_1和U_2$，要将其变换为单位圆的均匀采样公式$X=h_1(U_1,U_2),Y=h_2(U_1,U_2)$​。

那么同样的，需要找到变换后概率分布函数，求其反函数即可找到变换。

第一步：先求变换后的联合概率分布：根据单位圆上概率的归一化有：

$\int p(x,y)dxdy=1有p(x,y)=\frac{1}{\pi}$

换元为极坐标有：

$p(r,\theta)=rp(x,y)=\frac{r}{\pi}$

第二步：计算边缘概率密度分布

$F_r(R)=P(r<=R)=\int_{0}^{R}\int_{0}^{2\pi}p(r,\theta)d\theta dr=R^2$​

$F_\phi(\phi)=P(\theta<=\phi)=\int_{0}^{1}\int_{0}^{\phi}p(r,\theta)d\theta dr=\frac{\phi}{2\pi}$

第三步：求逆变换即可

$r = \sqrt{U_1}$

$\theta = 2\pi U_2$

待会直角坐标有

$X = \sqrt{U_1}cos(2\pi U_2)$

$Y = \sqrt{U_1}sin(2\pi U_2)$

验证：

```python
for i in range(10000):
    u1 = random.uniform(0, 1)
    u2 = random.uniform(0, 1)
    a.append(np.sqrt(u1) * math.cos(np.pi * 2 * u2))
    b.append(np.sqrt(u1) * math.sin(np.pi * 2 * u2))
```

![均匀圆盘采样2](D:\BaiduNetdiskWorkspace\games202\均匀圆盘采样2.png)

 

#### 实现PCF 

作业中要求的是采用圆盘滤波核进行采样。

- 采样函数需要传入一个随机种子

- 采样函数的输出是一个0到1范围内的二维随机数，作为uv坐标

- 因为是在texture2D是在shadowmap内进行采样，因此也就采样的二维

  $[0,1]^2$的区域也分为和shadowmap形状大小的网格

- 最后，根据确定的滤波半径R可以确定在滤波半径局部坐标内的偏移值，局部坐标的区域为$[0,R]^2$​​。只需将当前点坐标加上偏移值，就可以确定以当前查询点周围的滤波核内的区域点的坐标。

采用的泊松圆盘采样

![PCF_1_possion](D:\BaiduNetdiskWorkspace\games202\PCF_1_possion.png)

为啥会有磨砂感觉？

使用均匀圆盘采样之后

![PCF_2](D:\BaiduNetdiskWorkspace\games202\PCF_2.png)

把采样数增加试试，采样数增加到40

![PCF_3](D:\BaiduNetdiskWorkspace\games202\PCF_3.png)

没有改善啊。。。。

---

采样数100，卷积核大小20。噪点减少许多。

![PCF_4](D:\BaiduNetdiskWorkspace\games202\PCF_4.png)

### PCSS

半影范围（$w_{penumbra}$)

![21](D:\BaiduNetdiskWorkspace\games202\3\21.png)

- 半影范围越大，阴影也就越软，相当于PCF的范围也就越大
- 半影范围的性质：遮挡物Blocker越接近接受物Receiver,半影范围越小，滤波核大小越小，阴影越小
- 遮挡物Blocker越接近光源Light，半影范围越大，阴影也就越软，滤波核也就越大
- 上述公式未知量就是$d_{blocker}$ :如何计算?并不是我想的，计算整个模型(模型作为阻挡物）的$d_{blcoker}$的平均。而是**对于一个Shading Point,从当前的shading point连向光源light,这个方向上击中阻挡物的一点P，取P点的一片区域（泊松圆盘采样），计算这片区域的平均$d_{blocker}$**。基本思路和计算PCF是一样的，都是采样中心点周围一片区域以计算平均深度作为这一点的$d_{blocker}$​
- 注意这里的$d_{receiver}$​可以看作从光源视角下的z深度坐标即可，也就是每个着色点的深度信息

![PCSS](D:\BaiduNetdiskWorkspace\games202\PCSS.png)

还是比较明显的硬阴影与软阴影。

### 代码

```GLSL
float findBlocker( sampler2D shadowMap,  vec2 uv, float zReceiver ) {
	// zReceiver是接收物的深度(也就是shading point的深度)

  int numBlockers = 0;
  float blockerDepthSum = 0.0;
  float filterStride = 20.0;
  float filterRadius = 1.0 / SHADOWMAP_SIZE * filterStride;

  // 采样
  poissonDiskSamples(uv);

  // 判断是否阻挡并计算平均深度
  for(int i = 0; i < BLOCKER_SEARCH_NUM_SAMPLES; i ++)
  {
    vec2 offset = poissonDisk[i] * filterRadius ;
    vec2 sampleuv = uv + offset;
    float shadow_depth = unpack(texture2D(shadowMap, sampleuv));
    if(shadow_depth + EPS < zReceiver)
    {
      // 接收物在阴影中
      numBlockers++;
      blockerDepthSum += shadow_depth;
    }
  }

  // 计算平均深度并返回
  if(numBlockers == 0)
  {
    // 无阻挡
    return 1.0;
  }

  float avgBlockerDepth = blockerDepthSum / float(numBlockers);
  return avgBlockerDepth;
}

float PCF(sampler2D shadowMap, vec4 coords) {
  // 采样
  poissonDiskSamples(coords.xy);
  // 确定滤波核大小
  float filterStride = 20.0;
  // 先提前处理filterRadius
  float filterRadius = 1.0 / SHADOWMAP_SIZE * filterStride;
  // 统计有多少不在阴影里(因为不在阴影里确定为1.0)
  int numOutShadow = 0;
  for(int i = 0; i < NUM_SAMPLES; i++)
  {
    vec2 offset = poissonDisk[i] * filterRadius ;
    vec2 sampleuv = coords.xy + offset;
    float shadow_depth = unpack(texture2D(shadowMap, sampleuv));
    float current_depth = coords.z;
    if(current_depth < shadow_depth + EPS)
    {
      numOutShadow++;
    }
  }
  // 最后确定visibility
  float visibility = float(numOutShadow) / float(NUM_SAMPLES);
  return visibility;
}

float PCSS(sampler2D shadowMap, vec4 coords){

  // STEP 1: avgblocker depth
  float avgBlockerDepth = findBlocker(shadowMap, coords.xy, coords.z);
  // 光源大小
  float w_light = 1.0;
  float d_receiver = coords.z;

  // STEP 2: penumbra size
  float penumbraSize = (d_receiver - avgBlockerDepth) * w_light / avgBlockerDepth;

  // STEP 3: filtering(PCF)
  // 根据确定的半影范围确定滤波核大小
  float filterStride = 20.0;
  // 动态调整滤波半径
  float filterRadius = (1.0 / SHADOWMAP_SIZE * filterStride) * penumbraSize;
  int numOutShadow = 0;
  float current_depth = coords.z;

  // 采样
  poissonDiskSamples(coords.xy);
  for(int i = 0; i < NUM_SAMPLES; i ++)
  {
    vec2 offset = poissonDisk[i] * filterRadius  ;
    vec2 sampleuv = coords.xy + offset;
    float shadow_depth = unpack(texture2D(shadowMap, sampleuv));
    if(current_depth <= shadow_depth + EPS)
    {
      numOutShadow++;
    }
  }

  // STEP 4: visibility
  float visibility = float(numOutShadow) / float(NUM_SAMPLES);
  return visibility;
}


float useShadowMap(sampler2D shadowMap, vec4 shadowCoord, float c, float filterRadius){
  // float shadow_depth = unpack(texture2D(shadowMap, shadowCoord.xy));
  // float current_depth = shadowCoord.z;
  // if(shadow_depth + EPS < current_depth )
  // {
  //   // 现在从camera看到的深度大于shadowmap中存储的深度，说明被遮挡，返回0.0
  //   return 0.0;
  // }
  // else
  // {
  //   return 1.0;
  // }

  // 使用自适应的bias
  float shadow_depth = unpack(texture2D(shadowMap, shadowCoord.xy));
  float current_depth = shadowCoord.z;
  float bias = get_shadow_bias(c , filterRadius);
  if(current_depth - bias >= shadow_depth + EPS)
  {
    return 0.0;
  }
  else
  {
    return 1.0;
  }
}

vec3 blinnPhong() {
  vec3 color = texture2D(uSampler, vTextureCoord).rgb;
  color = pow(color, vec3(2.2));

  vec3 ambient = 0.05 * color;

  vec3 lightDir = normalize(uLightPos);
  vec3 normal = normalize(vNormal);
  float diff = max(dot(lightDir, normal), 0.0);
  vec3 light_atten_coff =
      uLightIntensity / pow(length(uLightPos - vFragPos), 2.0);
  vec3 diffuse = diff * light_atten_coff * color;

  vec3 viewDir = normalize(uCameraPos - vFragPos);
  vec3 halfDir = normalize((lightDir + viewDir));
  float spec = pow(max(dot(halfDir, normal), 0.0), 32.0);
  vec3 specular = uKs * light_atten_coff * spec;

  vec3 radiance = (ambient + diffuse + specular);
  vec3 phongColor = pow(radiance, vec3(1.0 / 2.2));
  return phongColor;
}

void main(void) {

  // vPositionFromLight为光源为视点的空间下的裁剪坐标，除以w为NDC坐标
  vec3 shadowCoord = vPositionFromLight.xyz / vPositionFromLight.w;
  // 把[-1,1]的裁剪坐标转化为[0,1]坐标
  shadowCoord.xyz = (shadowCoord.xyz + 1.0) / 2.0;
  float visibility;
  //visibility = useShadowMap(uShadowMap, vec4(shadowCoord, 1.0), 0.4, 0.7);
  //visibility = PCF(uShadowMap, vec4(shadowCoord, 1.0));
  visibility = PCSS(uShadowMap, vec4(shadowCoord, 1.0));

  vec3 phongColor = blinnPhong();

  gl_FragColor = vec4(phongColor * visibility, 1.0);
  // gl_FragColor = vec4(phongColor, 1.0);
}
```



